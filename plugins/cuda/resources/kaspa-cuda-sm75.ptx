//
// Generated by NVIDIA NVVM Compiler
//
// Compiler Build ID: CL-30672275
// Cuda compilation tools, release 11.5, V11.5.119
// Based on NVVM 7.0.1
//

.version 7.5
.target sm_75
.address_size 64

	// .globl	heavy_hash
.global .align 1 .b8 rho[24] = {1, 3, 6, 10, 15, 21, 28, 36, 45, 55, 2, 14, 27, 41, 56, 8, 25, 43, 62, 18, 39, 61, 20, 44};
.global .align 1 .b8 pi[24] = {10, 7, 11, 17, 18, 3, 5, 16, 8, 21, 24, 4, 15, 23, 19, 13, 12, 2, 20, 14, 22, 9, 6, 1};
.global .align 8 .b8 RC[192] = {1, 0, 0, 0, 0, 0, 0, 0, 130, 128, 0, 0, 0, 0, 0, 0, 138, 128, 0, 0, 0, 0, 0, 128, 0, 128, 0, 128, 0, 0, 0, 128, 139, 128, 0, 0, 0, 0, 0, 0, 1, 0, 0, 128, 0, 0, 0, 0, 129, 128, 0, 128, 0, 0, 0, 128, 9, 128, 0, 0, 0, 0, 0, 128, 138, 0, 0, 0, 0, 0, 0, 0, 136, 0, 0, 0, 0, 0, 0, 0, 9, 128, 0, 128, 0, 0, 0, 0, 10, 0, 0, 128, 0, 0, 0, 0, 139, 128, 0, 128, 0, 0, 0, 0, 139, 0, 0, 0, 0, 0, 0, 128, 137, 128, 0, 0, 0, 0, 0, 128, 3, 128, 0, 0, 0, 0, 0, 128, 2, 128, 0, 0, 0, 0, 0, 128, 128, 0, 0, 0, 0, 0, 0, 128, 10, 128, 0, 0, 0, 0, 0, 0, 10, 0, 0, 128, 0, 0, 0, 128, 129, 128, 0, 128, 0, 0, 0, 128, 128, 128, 0, 0, 0, 0, 0, 128, 1, 0, 0, 128, 0, 0, 0, 0, 8, 128, 0, 128, 0, 0, 0, 128};
.global .align 8 .b8 _ZZ15xoshiro256_jumpP10ulonglong4E4JUMP[32] = {186, 10, 253, 60, 211, 198, 14, 24, 44, 57, 201, 240, 102, 18, 166, 213, 170, 201, 63, 224, 24, 38, 88, 169, 28, 102, 177, 41, 69, 220, 171, 57};
.global .align 8 .b8 _ZZ20xoshiro256_long_jumpP10ulonglong4E9LONG_JUMP[32] = {191, 203, 253, 254, 62, 93, 225, 118, 179, 47, 82, 28, 68, 78, 0, 197, 65, 226, 78, 133, 105, 0, 113, 119, 53, 230, 203, 42, 176, 155, 16, 57};
.const .align 1 .b8 matrix[4096];
.const .align 8 .b8 hash_header[72];
.const .align 8 .b8 target[32];
.const .align 1 .b8 powP[200] = {61, 216, 246, 161, 13, 255, 60, 17, 60, 126, 2, 183, 85, 136, 191, 41, 210, 68, 251, 14, 114, 46, 95, 30, 160, 105, 152, 245, 163, 164, 165, 27, 101, 45, 94, 135, 202, 175, 47, 123, 70, 226, 220, 41, 214, 97, 239, 74, 16, 91, 65, 173, 30, 152, 58, 24, 156, 194, 155, 120, 12, 246, 107, 119, 64, 49, 102, 136, 51, 241, 235, 248, 240, 95, 40, 67, 60, 28, 101, 46, 10, 74, 241, 64, 5, 7, 150, 15, 82, 145, 41, 91, 135, 103, 227, 68, 21, 55, 177, 37, 164, 241, 112, 236, 137, 218, 233, 130, 143, 93, 200, 230, 35, 178, 180, 133, 31, 96, 26, 178, 70, 106, 163, 100, 144, 84, 133, 52, 26, 133, 47, 122, 28, 221, 6, 15, 66, 177, 59, 86, 29, 2, 162, 193, 228, 104, 22, 69, 228, 229, 29, 186, 141, 95, 9, 5, 65, 87, 2, 209, 74, 207, 206, 155, 132, 78, 202, 137, 219, 46, 116, 168, 39, 148, 176, 72, 114, 82, 139, 231, 156, 206, 252, 177, 188, 165, 175, 130, 207, 41, 17, 93, 131, 67, 130, 111, 120, 124, 185, 2};
.const .align 1 .b8 heavyP[200] = {9, 133, 36, 178, 82, 76, 215, 58, 22, 66, 159, 47, 14, 155, 98, 121, 238, 248, 199, 22, 72, 255, 20, 122, 152, 100, 5, 128, 76, 95, 167, 17, 218, 206, 238, 68, 223, 224, 32, 231, 105, 64, 243, 20, 46, 216, 199, 114, 186, 53, 137, 147, 42, 255, 0, 193, 98, 196, 15, 37, 64, 144, 33, 94, 72, 106, 207, 13, 166, 249, 57, 128, 12, 61, 42, 121, 159, 170, 188, 160, 38, 162, 169, 208, 93, 192, 49, 244, 63, 140, 193, 84, 195, 76, 31, 211, 61, 204, 105, 167, 1, 125, 107, 108, 228, 147, 36, 86, 211, 91, 198, 46, 68, 176, 205, 153, 58, 75, 247, 78, 176, 242, 52, 84, 131, 134, 76, 119, 22, 148, 188, 54, 176, 97, 233, 7, 7, 204, 101, 119, 177, 29, 143, 126, 57, 109, 196, 186, 128, 219, 143, 234, 88, 202, 52, 123, 211, 242, 146, 185, 87, 185, 129, 132, 4, 197, 118, 199, 46, 194, 18, 81, 103, 159, 195, 71, 10, 12, 41, 181, 157, 57, 187, 146, 21, 198, 159, 47, 49, 224, 154, 84, 53, 218, 185, 16, 125, 50, 25, 22};

.visible .entry heavy_hash(
	.param .u64 heavy_hash_param_0,
	.param .u64 heavy_hash_param_1,
	.param .u64 heavy_hash_param_2
)
{
	.local .align 8 .b8 	__local_depot0[32];
	.reg .b64 	%SP;
	.reg .b64 	%SPL;
	.reg .pred 	%p<16>;
	.reg .b16 	%rs<13>;
	.reg .b32 	%r<688>;
	.reg .b64 	%rd<555>;


	mov.u64 	%SPL, __local_depot0;
	ld.param.u64 	%rd132, [heavy_hash_param_0];
	ld.param.u64 	%rd131, [heavy_hash_param_1];
	ld.param.u64 	%rd133, [heavy_hash_param_2];
	cvta.to.global.u64 	%rd1, %rd133;
	add.u64 	%rd2, %SPL, 0;
	mov.u32 	%r23, %ntid.x;
	mov.u32 	%r24, %ctaid.x;
	mov.u32 	%r25, %tid.x;
	mad.lo.s32 	%r26, %r24, %r23, %r25;
	cvt.s64.s32 	%rd3, %r26;
	setp.ge.u64 	%p6, %rd3, %rd132;
	@%p6 bra 	$L__BB0_18;

	cvt.u32.u64 	%r27, %rd3;
	setp.ne.s32 	%p7, %r27, 0;
	@%p7 bra 	$L__BB0_3;

	mov.u64 	%rd135, 0;
	st.global.u64 	[%rd1], %rd135;

$L__BB0_3:
	cvta.to.global.u64 	%rd152, %rd131;
	shl.b64 	%rd153, %rd3, 5;
	add.s64 	%rd154, %rd152, %rd153;
	ld.global.v2.u64 	{%rd155, %rd156}, [%rd154];
	mov.u32 	%r685, 0;
	mul.lo.s64 	%rd159, %rd156, 5;
	{
	.reg .b64 %lhs;
	.reg .b64 %rhs;
	shl.b64 	%lhs, %rd159, 7;
	shr.b64 	%rhs, %rd159, 57;
	add.u64 	%rd160, %lhs, %rhs;
	}
	mul.lo.s64 	%rd4, %rd160, 9;
	shl.b64 	%rd161, %rd156, 17;
	ld.global.v2.u64 	{%rd162, %rd163}, [%rd154+16];
	xor.b64  	%rd166, %rd162, %rd155;
	xor.b64  	%rd167, %rd163, %rd156;
	xor.b64  	%rd168, %rd156, %rd166;
	xor.b64  	%rd169, %rd155, %rd167;
	st.global.v2.u64 	[%rd154], {%rd169, %rd168};
	{
	.reg .b32 %dummy;
	mov.b64 	{%r29,%dummy}, %rd167;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r30}, %rd167;
	}
	shf.r.wrap.b32 	%r31, %r30, %r29, 19;
	shf.r.wrap.b32 	%r32, %r29, %r30, 19;
	mov.b64 	%rd170, {%r32, %r31};
	xor.b64  	%rd171, %rd166, %rd161;
	st.global.v2.u64 	[%rd154+16], {%rd171, %rd170};
	ld.const.u64 	%rd172, [hash_header];
	xor.b64  	%rd526, %rd172, 1242148031264380989;
	ld.const.u64 	%rd173, [hash_header+8];
	xor.b64  	%rd521, %rd173, 3008272977830772284;
	ld.const.u64 	%rd174, [hash_header+16];
	xor.b64  	%rd516, %rd174, 2188519011337848018;
	ld.const.u64 	%rd175, [hash_header+24];
	xor.b64  	%rd511, %rd175, 1992179434288343456;
	ld.const.u64 	%rd176, [hash_header+32];
	xor.b64  	%rd506, %rd176, 8876506674959887717;
	ld.const.u64 	%rd177, [hash_header+40];
	xor.b64  	%rd525, %rd177, 5399642050693751366;
	ld.const.u64 	%rd178, [hash_header+48];
	xor.b64  	%rd520, %rd178, 1745875063082670864;
	ld.const.u64 	%rd179, [hash_header+56];
	xor.b64  	%rd515, %rd179, 8605242046444978844;
	ld.const.u64 	%rd180, [hash_header+64];
	xor.b64  	%rd510, %rd180, -510048929142394560;
	xor.b64  	%rd505, %rd4, 3343109343542796272;
	mov.u64 	%rd524, 1123092876221303306;
	mov.u64 	%rd523, 3784524041015224902;
	mov.u64 	%rd522, -8517909413761200310;
	mov.u64 	%rd519, 4963925045340115282;
	mov.u64 	%rd518, 1082795874807940378;
	mov.u64 	%rd517, 5237849264682708699;
	mov.u64 	%rd514, -1409360996057663723;
	mov.u64 	%rd513, -4494027153138273982;
	mov.u64 	%rd512, -5621391061570334094;
	mov.u64 	%rd509, -1817099578685924727;
	mov.u64 	%rd508, -5035616039755945756;
	mov.u64 	%rd507, 6706187291358897596;
	mov.u64 	%rd504, -5613068297060437469;
	mov.u64 	%rd503, -3386048033060200563;
	mov.u64 	%rd502, 196324915476054915;
	mov.u64 	%rd501, RC;

$L__BB0_4:
	xor.b64  	%rd181, %rd525, %rd526;
	xor.b64  	%rd182, %rd181, %rd524;
	xor.b64  	%rd183, %rd182, %rd523;
	xor.b64  	%rd184, %rd183, %rd522;
	xor.b64  	%rd185, %rd520, %rd521;
	xor.b64  	%rd186, %rd185, %rd519;
	xor.b64  	%rd187, %rd186, %rd518;
	xor.b64  	%rd188, %rd187, %rd517;
	xor.b64  	%rd189, %rd515, %rd516;
	xor.b64  	%rd190, %rd189, %rd514;
	xor.b64  	%rd191, %rd190, %rd513;
	xor.b64  	%rd192, %rd191, %rd512;
	xor.b64  	%rd193, %rd510, %rd511;
	xor.b64  	%rd194, %rd193, %rd509;
	xor.b64  	%rd195, %rd194, %rd508;
	xor.b64  	%rd196, %rd195, %rd507;
	xor.b64  	%rd197, %rd505, %rd506;
	xor.b64  	%rd198, %rd197, %rd504;
	xor.b64  	%rd199, %rd198, %rd503;
	xor.b64  	%rd200, %rd199, %rd502;
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r33}, %rd188;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%r34,%dummy}, %rd188;
	}
	shf.l.wrap.b32 	%r35, %r34, %r33, 1;
	shf.l.wrap.b32 	%r36, %r33, %r34, 1;
	mov.b64 	%rd201, {%r36, %r35};
	xor.b64  	%rd202, %rd200, %rd201;
	xor.b64  	%rd203, %rd202, %rd526;
	xor.b64  	%rd204, %rd525, %rd202;
	xor.b64  	%rd205, %rd524, %rd202;
	xor.b64  	%rd206, %rd523, %rd202;
	xor.b64  	%rd207, %rd522, %rd202;
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r37}, %rd192;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%r38,%dummy}, %rd192;
	}
	shf.l.wrap.b32 	%r39, %r38, %r37, 1;
	shf.l.wrap.b32 	%r40, %r37, %r38, 1;
	mov.b64 	%rd208, {%r40, %r39};
	xor.b64  	%rd209, %rd208, %rd184;
	xor.b64  	%rd210, %rd521, %rd209;
	xor.b64  	%rd211, %rd520, %rd209;
	xor.b64  	%rd212, %rd519, %rd209;
	xor.b64  	%rd213, %rd518, %rd209;
	xor.b64  	%rd214, %rd517, %rd209;
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r41}, %rd196;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%r42,%dummy}, %rd196;
	}
	shf.l.wrap.b32 	%r43, %r42, %r41, 1;
	shf.l.wrap.b32 	%r44, %r41, %r42, 1;
	mov.b64 	%rd215, {%r44, %r43};
	xor.b64  	%rd216, %rd215, %rd188;
	xor.b64  	%rd217, %rd516, %rd216;
	xor.b64  	%rd218, %rd515, %rd216;
	xor.b64  	%rd219, %rd514, %rd216;
	xor.b64  	%rd220, %rd513, %rd216;
	xor.b64  	%rd221, %rd512, %rd216;
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r45}, %rd200;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%r46,%dummy}, %rd200;
	}
	shf.l.wrap.b32 	%r47, %r46, %r45, 1;
	shf.l.wrap.b32 	%r48, %r45, %r46, 1;
	mov.b64 	%rd222, {%r48, %r47};
	xor.b64  	%rd223, %rd222, %rd192;
	xor.b64  	%rd224, %rd511, %rd223;
	xor.b64  	%rd225, %rd510, %rd223;
	xor.b64  	%rd226, %rd509, %rd223;
	xor.b64  	%rd227, %rd508, %rd223;
	xor.b64  	%rd228, %rd507, %rd223;
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r49}, %rd184;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%r50,%dummy}, %rd184;
	}
	shf.l.wrap.b32 	%r51, %r50, %r49, 1;
	shf.l.wrap.b32 	%r52, %r49, %r50, 1;
	mov.b64 	%rd229, {%r52, %r51};
	xor.b64  	%rd230, %rd196, %rd229;
	xor.b64  	%rd231, %rd506, %rd230;
	xor.b64  	%rd232, %rd505, %rd230;
	xor.b64  	%rd233, %rd504, %rd230;
	xor.b64  	%rd234, %rd503, %rd230;
	xor.b64  	%rd235, %rd502, %rd230;
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r53}, %rd210;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%r54,%dummy}, %rd210;
	}
	shf.l.wrap.b32 	%r55, %r54, %r53, 1;
	shf.l.wrap.b32 	%r56, %r53, %r54, 1;
	mov.b64 	%rd236, {%r56, %r55};
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r57}, %rd205;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%r58,%dummy}, %rd205;
	}
	shf.l.wrap.b32 	%r59, %r58, %r57, 3;
	shf.l.wrap.b32 	%r60, %r57, %r58, 3;
	mov.b64 	%rd237, {%r60, %r59};
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r61}, %rd218;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%r62,%dummy}, %rd218;
	}
	shf.l.wrap.b32 	%r63, %r62, %r61, 6;
	shf.l.wrap.b32 	%r64, %r61, %r62, 6;
	mov.b64 	%rd238, {%r64, %r63};
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r65}, %rd212;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%r66,%dummy}, %rd212;
	}
	shf.l.wrap.b32 	%r67, %r66, %r65, 10;
	shf.l.wrap.b32 	%r68, %r65, %r66, 10;
	mov.b64 	%rd239, {%r68, %r67};
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r69}, %rd220;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%r70,%dummy}, %rd220;
	}
	shf.l.wrap.b32 	%r71, %r70, %r69, 15;
	shf.l.wrap.b32 	%r72, %r69, %r70, 15;
	mov.b64 	%rd240, {%r72, %r71};
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r73}, %rd227;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%r74,%dummy}, %rd227;
	}
	shf.l.wrap.b32 	%r75, %r74, %r73, 21;
	shf.l.wrap.b32 	%r76, %r73, %r74, 21;
	mov.b64 	%rd241, {%r76, %r75};
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r77}, %rd224;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%r78,%dummy}, %rd224;
	}
	shf.l.wrap.b32 	%r79, %r78, %r77, 28;
	shf.l.wrap.b32 	%r80, %r77, %r78, 28;
	mov.b64 	%rd242, {%r80, %r79};
	{
	.reg .b32 %dummy;
	mov.b64 	{%r81,%dummy}, %rd204;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r82}, %rd204;
	}
	shf.r.wrap.b32 	%r83, %r82, %r81, 28;
	shf.r.wrap.b32 	%r84, %r81, %r82, 28;
	mov.b64 	%rd243, {%r84, %r83};
	{
	.reg .b32 %dummy;
	mov.b64 	{%r85,%dummy}, %rd213;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r86}, %rd213;
	}
	shf.r.wrap.b32 	%r87, %r86, %r85, 19;
	shf.r.wrap.b32 	%r88, %r85, %r86, 19;
	mov.b64 	%rd244, {%r88, %r87};
	{
	.reg .b32 %dummy;
	mov.b64 	{%r89,%dummy}, %rd225;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r90}, %rd225;
	}
	shf.r.wrap.b32 	%r91, %r90, %r89, 9;
	shf.r.wrap.b32 	%r92, %r89, %r90, 9;
	mov.b64 	%rd245, {%r92, %r91};
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r93}, %rd214;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%r94,%dummy}, %rd214;
	}
	shf.l.wrap.b32 	%r95, %r94, %r93, 2;
	shf.l.wrap.b32 	%r96, %r93, %r94, 2;
	mov.b64 	%rd246, {%r96, %r95};
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r97}, %rd235;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%r98,%dummy}, %rd235;
	}
	shf.l.wrap.b32 	%r99, %r98, %r97, 14;
	shf.l.wrap.b32 	%r100, %r97, %r98, 14;
	mov.b64 	%rd247, {%r100, %r99};
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r101}, %rd231;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%r102,%dummy}, %rd231;
	}
	shf.l.wrap.b32 	%r103, %r102, %r101, 27;
	shf.l.wrap.b32 	%r104, %r101, %r102, 27;
	mov.b64 	%rd248, {%r104, %r103};
	{
	.reg .b32 %dummy;
	mov.b64 	{%r105,%dummy}, %rd206;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r106}, %rd206;
	}
	shf.r.wrap.b32 	%r107, %r106, %r105, 23;
	shf.r.wrap.b32 	%r108, %r105, %r106, 23;
	mov.b64 	%rd249, {%r108, %r107};
	{
	.reg .b32 %dummy;
	mov.b64 	{%r109,%dummy}, %rd228;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r110}, %rd228;
	}
	shf.r.wrap.b32 	%r111, %r110, %r109, 8;
	shf.r.wrap.b32 	%r112, %r109, %r110, 8;
	mov.b64 	%rd250, {%r112, %r111};
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r113}, %rd234;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%r114,%dummy}, %rd234;
	}
	shf.l.wrap.b32 	%r115, %r114, %r113, 8;
	shf.l.wrap.b32 	%r116, %r113, %r114, 8;
	mov.b64 	%rd251, {%r116, %r115};
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r117}, %rd226;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%r118,%dummy}, %rd226;
	}
	shf.l.wrap.b32 	%r119, %r118, %r117, 25;
	shf.l.wrap.b32 	%r120, %r117, %r118, 25;
	mov.b64 	%rd252, {%r120, %r119};
	{
	.reg .b32 %dummy;
	mov.b64 	{%r121,%dummy}, %rd219;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r122}, %rd219;
	}
	shf.r.wrap.b32 	%r123, %r122, %r121, 21;
	shf.r.wrap.b32 	%r124, %r121, %r122, 21;
	mov.b64 	%rd253, {%r124, %r123};
	{
	.reg .b32 %dummy;
	mov.b64 	{%r125,%dummy}, %rd217;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r126}, %rd217;
	}
	shf.r.wrap.b32 	%r127, %r126, %r125, 2;
	shf.r.wrap.b32 	%r128, %r125, %r126, 2;
	mov.b64 	%rd254, {%r128, %r127};
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r129}, %rd207;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%r130,%dummy}, %rd207;
	}
	shf.l.wrap.b32 	%r131, %r130, %r129, 18;
	shf.l.wrap.b32 	%r132, %r129, %r130, 18;
	mov.b64 	%rd255, {%r132, %r131};
	{
	.reg .b32 %dummy;
	mov.b64 	{%r133,%dummy}, %rd233;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r134}, %rd233;
	}
	shf.r.wrap.b32 	%r135, %r134, %r133, 25;
	shf.r.wrap.b32 	%r136, %r133, %r134, 25;
	mov.b64 	%rd256, {%r136, %r135};
	{
	.reg .b32 %dummy;
	mov.b64 	{%r137,%dummy}, %rd221;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r138}, %rd221;
	}
	shf.r.wrap.b32 	%r139, %r138, %r137, 3;
	shf.r.wrap.b32 	%r140, %r137, %r138, 3;
	mov.b64 	%rd257, {%r140, %r139};
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r141}, %rd232;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%r142,%dummy}, %rd232;
	}
	shf.l.wrap.b32 	%r143, %r142, %r141, 20;
	shf.l.wrap.b32 	%r144, %r141, %r142, 20;
	mov.b64 	%rd258, {%r144, %r143};
	{
	.reg .b32 %dummy;
	mov.b64 	{%r145,%dummy}, %rd211;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r146}, %rd211;
	}
	shf.r.wrap.b32 	%r147, %r146, %r145, 20;
	shf.r.wrap.b32 	%r148, %r145, %r146, 20;
	mov.b64 	%rd259, {%r148, %r147};
	not.b64 	%rd260, %rd259;
	and.b64  	%rd261, %rd253, %rd260;
	xor.b64  	%rd262, %rd261, %rd203;
	not.b64 	%rd263, %rd253;
	and.b64  	%rd264, %rd241, %rd263;
	xor.b64  	%rd521, %rd264, %rd259;
	not.b64 	%rd265, %rd241;
	and.b64  	%rd266, %rd247, %rd265;
	xor.b64  	%rd516, %rd266, %rd253;
	not.b64 	%rd267, %rd247;
	and.b64  	%rd268, %rd203, %rd267;
	xor.b64  	%rd511, %rd268, %rd241;
	not.b64 	%rd269, %rd203;
	and.b64  	%rd270, %rd259, %rd269;
	xor.b64  	%rd506, %rd247, %rd270;
	not.b64 	%rd271, %rd258;
	and.b64  	%rd272, %rd237, %rd271;
	xor.b64  	%rd525, %rd272, %rd242;
	not.b64 	%rd273, %rd237;
	and.b64  	%rd274, %rd244, %rd273;
	xor.b64  	%rd520, %rd274, %rd258;
	not.b64 	%rd275, %rd244;
	and.b64  	%rd276, %rd257, %rd275;
	xor.b64  	%rd515, %rd276, %rd237;
	not.b64 	%rd277, %rd257;
	and.b64  	%rd278, %rd242, %rd277;
	xor.b64  	%rd510, %rd278, %rd244;
	not.b64 	%rd279, %rd242;
	and.b64  	%rd280, %rd258, %rd279;
	xor.b64  	%rd505, %rd257, %rd280;
	not.b64 	%rd281, %rd238;
	and.b64  	%rd282, %rd252, %rd281;
	xor.b64  	%rd524, %rd282, %rd236;
	not.b64 	%rd283, %rd252;
	and.b64  	%rd284, %rd251, %rd283;
	xor.b64  	%rd519, %rd284, %rd238;
	not.b64 	%rd285, %rd251;
	and.b64  	%rd286, %rd255, %rd285;
	xor.b64  	%rd514, %rd286, %rd252;
	not.b64 	%rd287, %rd255;
	and.b64  	%rd288, %rd236, %rd287;
	xor.b64  	%rd509, %rd288, %rd251;
	not.b64 	%rd289, %rd236;
	and.b64  	%rd290, %rd238, %rd289;
	xor.b64  	%rd504, %rd255, %rd290;
	not.b64 	%rd291, %rd243;
	and.b64  	%rd292, %rd239, %rd291;
	xor.b64  	%rd523, %rd292, %rd248;
	not.b64 	%rd293, %rd239;
	and.b64  	%rd294, %rd240, %rd293;
	xor.b64  	%rd518, %rd294, %rd243;
	not.b64 	%rd295, %rd240;
	and.b64  	%rd296, %rd250, %rd295;
	xor.b64  	%rd513, %rd296, %rd239;
	not.b64 	%rd297, %rd250;
	and.b64  	%rd298, %rd248, %rd297;
	xor.b64  	%rd508, %rd298, %rd240;
	not.b64 	%rd299, %rd248;
	and.b64  	%rd300, %rd243, %rd299;
	xor.b64  	%rd503, %rd250, %rd300;
	not.b64 	%rd301, %rd245;
	and.b64  	%rd302, %rd256, %rd301;
	xor.b64  	%rd522, %rd302, %rd254;
	not.b64 	%rd303, %rd256;
	and.b64  	%rd304, %rd249, %rd303;
	xor.b64  	%rd517, %rd304, %rd245;
	not.b64 	%rd305, %rd249;
	and.b64  	%rd306, %rd246, %rd305;
	xor.b64  	%rd512, %rd306, %rd256;
	not.b64 	%rd307, %rd246;
	and.b64  	%rd308, %rd254, %rd307;
	xor.b64  	%rd507, %rd308, %rd249;
	not.b64 	%rd309, %rd254;
	and.b64  	%rd310, %rd245, %rd309;
	xor.b64  	%rd502, %rd246, %rd310;
	ld.global.nc.u64 	%rd311, [%rd501];
	xor.b64  	%rd526, %rd262, %rd311;
	add.s64 	%rd501, %rd501, 8;
	add.s32 	%r685, %r685, 1;
	setp.ne.s32 	%p8, %r685, 24;
	@%p8 bra 	$L__BB0_4;

	st.local.u64 	[%rd2], %rd526;
	st.local.u64 	[%rd2+8], %rd521;
	st.local.u64 	[%rd2+16], %rd516;
	st.local.u64 	[%rd2+24], %rd511;
	cvt.u16.u64 	%rs1, %rd526;
	and.b16  	%rs2, %rs1, 240;
	shr.u16 	%rs3, %rs2, 4;
	cvt.u32.u64 	%r150, %rd526;
	shr.u32 	%r151, %r150, 12;
	cvt.u32.u16 	%r152, %rs3;
	and.b32  	%r153, %r150, 15;
	prmt.b32 	%r154, %r153, %r152, 30212;
	shl.b32 	%r155, %r150, 4;
	and.b32  	%r156, %r155, 983040;
	or.b32  	%r157, %r154, %r156;
	shl.b32 	%r158, %r150, 16;
	and.b32  	%r159, %r158, 251658240;
	or.b32  	%r3, %r157, %r159;
	bfe.u32 	%r160, %r150, 20, 4;
	bfe.u32 	%r161, %r150, 16, 4;
	bfi.b32 	%r162, %r161, %r160, 8, 4;
	and.b32  	%r163, %r151, 983040;
	or.b32  	%r164, %r162, %r163;
	and.b32  	%r165, %r150, 251658240;
	or.b32  	%r4, %r164, %r165;
	shr.u64 	%rd313, %rd526, 32;
	cvt.u32.u64 	%r166, %rd313;
	shr.u64 	%rd314, %rd526, 36;
	cvt.u32.u64 	%r167, %rd314;
	and.b32  	%r168, %r167, 15;
	and.b32  	%r169, %r166, 15;
	shr.u64 	%rd315, %rd526, 40;
	cvt.u32.u64 	%r170, %rd315;
	shr.u64 	%rd316, %rd526, 44;
	cvt.u32.u64 	%r171, %rd316;
	bfi.b32 	%r172, %r169, %r168, 8, 4;
	shl.b32 	%r173, %r171, 16;
	and.b32  	%r174, %r173, 983040;
	or.b32  	%r175, %r172, %r174;
	shl.b32 	%r176, %r170, 24;
	and.b32  	%r177, %r176, 251658240;
	or.b32  	%r5, %r175, %r177;
	shr.u64 	%rd317, %rd526, 48;
	cvt.u32.u64 	%r178, %rd317;
	shr.u64 	%rd318, %rd526, 52;
	cvt.u32.u64 	%r179, %rd318;
	and.b32  	%r180, %r179, 15;
	and.b32  	%r181, %r178, 15;
	shr.u64 	%rd319, %rd526, 56;
	cvt.u32.u64 	%r182, %rd319;
	bfi.b32 	%r183, %r181, %r180, 8, 4;
	and.b32  	%r184, %r171, 983040;
	or.b32  	%r185, %r183, %r184;
	shl.b32 	%r186, %r182, 24;
	and.b32  	%r187, %r186, 251658240;
	or.b32  	%r6, %r185, %r187;
	cvt.u16.u64 	%rs4, %rd521;
	and.b16  	%rs5, %rs4, 240;
	shr.u16 	%rs6, %rs5, 4;
	cvt.u32.u64 	%r188, %rd521;
	shr.u32 	%r189, %r188, 12;
	cvt.u32.u16 	%r190, %rs6;
	and.b32  	%r191, %r188, 15;
	prmt.b32 	%r192, %r191, %r190, 30212;
	shl.b32 	%r193, %r188, 4;
	and.b32  	%r194, %r193, 983040;
	or.b32  	%r195, %r192, %r194;
	shl.b32 	%r196, %r188, 16;
	and.b32  	%r197, %r196, 251658240;
	or.b32  	%r7, %r195, %r197;
	bfe.u32 	%r198, %r188, 20, 4;
	bfe.u32 	%r199, %r188, 16, 4;
	bfi.b32 	%r200, %r199, %r198, 8, 4;
	and.b32  	%r201, %r189, 983040;
	or.b32  	%r202, %r200, %r201;
	and.b32  	%r203, %r188, 251658240;
	or.b32  	%r8, %r202, %r203;
	shr.u64 	%rd320, %rd521, 32;
	cvt.u32.u64 	%r204, %rd320;
	shr.u64 	%rd321, %rd521, 36;
	cvt.u32.u64 	%r205, %rd321;
	and.b32  	%r206, %r205, 15;
	and.b32  	%r207, %r204, 15;
	shr.u64 	%rd322, %rd521, 40;
	cvt.u32.u64 	%r208, %rd322;
	shr.u64 	%rd323, %rd521, 44;
	cvt.u32.u64 	%r209, %rd323;
	bfi.b32 	%r210, %r207, %r206, 8, 4;
	shl.b32 	%r211, %r209, 16;
	and.b32  	%r212, %r211, 983040;
	or.b32  	%r213, %r210, %r212;
	shl.b32 	%r214, %r208, 24;
	and.b32  	%r215, %r214, 251658240;
	or.b32  	%r9, %r213, %r215;
	shr.u64 	%rd324, %rd521, 48;
	cvt.u32.u64 	%r216, %rd324;
	shr.u64 	%rd325, %rd521, 52;
	cvt.u32.u64 	%r217, %rd325;
	and.b32  	%r218, %r217, 15;
	and.b32  	%r219, %r216, 15;
	shr.u64 	%rd326, %rd521, 56;
	cvt.u32.u64 	%r220, %rd326;
	bfi.b32 	%r221, %r219, %r218, 8, 4;
	and.b32  	%r222, %r209, 983040;
	or.b32  	%r223, %r221, %r222;
	shl.b32 	%r224, %r220, 24;
	and.b32  	%r225, %r224, 251658240;
	or.b32  	%r10, %r223, %r225;
	cvt.u16.u64 	%rs7, %rd516;
	and.b16  	%rs8, %rs7, 240;
	shr.u16 	%rs9, %rs8, 4;
	cvt.u32.u64 	%r226, %rd516;
	shr.u32 	%r227, %r226, 12;
	cvt.u32.u16 	%r228, %rs9;
	and.b32  	%r229, %r226, 15;
	prmt.b32 	%r230, %r229, %r228, 30212;
	shl.b32 	%r231, %r226, 4;
	and.b32  	%r232, %r231, 983040;
	or.b32  	%r233, %r230, %r232;
	shl.b32 	%r234, %r226, 16;
	and.b32  	%r235, %r234, 251658240;
	or.b32  	%r11, %r233, %r235;
	bfe.u32 	%r236, %r226, 20, 4;
	bfe.u32 	%r237, %r226, 16, 4;
	bfi.b32 	%r238, %r237, %r236, 8, 4;
	and.b32  	%r239, %r227, 983040;
	or.b32  	%r240, %r238, %r239;
	and.b32  	%r241, %r226, 251658240;
	or.b32  	%r12, %r240, %r241;
	shr.u64 	%rd327, %rd516, 32;
	cvt.u32.u64 	%r242, %rd327;
	shr.u64 	%rd328, %rd516, 36;
	cvt.u32.u64 	%r243, %rd328;
	and.b32  	%r244, %r243, 15;
	and.b32  	%r245, %r242, 15;
	shr.u64 	%rd329, %rd516, 40;
	cvt.u32.u64 	%r246, %rd329;
	shr.u64 	%rd330, %rd516, 44;
	cvt.u32.u64 	%r247, %rd330;
	bfi.b32 	%r248, %r245, %r244, 8, 4;
	shl.b32 	%r249, %r247, 16;
	and.b32  	%r250, %r249, 983040;
	or.b32  	%r251, %r248, %r250;
	shl.b32 	%r252, %r246, 24;
	and.b32  	%r253, %r252, 251658240;
	or.b32  	%r13, %r251, %r253;
	shr.u64 	%rd331, %rd516, 48;
	cvt.u32.u64 	%r254, %rd331;
	shr.u64 	%rd332, %rd516, 52;
	cvt.u32.u64 	%r255, %rd332;
	and.b32  	%r256, %r255, 15;
	and.b32  	%r257, %r254, 15;
	shr.u64 	%rd333, %rd516, 56;
	cvt.u32.u64 	%r258, %rd333;
	bfi.b32 	%r259, %r257, %r256, 8, 4;
	and.b32  	%r260, %r247, 983040;
	or.b32  	%r261, %r259, %r260;
	shl.b32 	%r262, %r258, 24;
	and.b32  	%r263, %r262, 251658240;
	or.b32  	%r14, %r261, %r263;
	cvt.u16.u64 	%rs10, %rd511;
	and.b16  	%rs11, %rs10, 240;
	shr.u16 	%rs12, %rs11, 4;
	cvt.u32.u64 	%r264, %rd511;
	shr.u32 	%r265, %r264, 12;
	cvt.u32.u16 	%r266, %rs12;
	and.b32  	%r267, %r264, 15;
	prmt.b32 	%r268, %r267, %r266, 30212;
	shl.b32 	%r269, %r264, 4;
	and.b32  	%r270, %r269, 983040;
	or.b32  	%r271, %r268, %r270;
	shl.b32 	%r272, %r264, 16;
	and.b32  	%r273, %r272, 251658240;
	or.b32  	%r15, %r271, %r273;
	bfe.u32 	%r274, %r264, 20, 4;
	bfe.u32 	%r275, %r264, 16, 4;
	bfi.b32 	%r276, %r275, %r274, 8, 4;
	and.b32  	%r277, %r265, 983040;
	or.b32  	%r278, %r276, %r277;
	and.b32  	%r279, %r264, 251658240;
	or.b32  	%r16, %r278, %r279;
	shr.u64 	%rd334, %rd511, 32;
	cvt.u32.u64 	%r280, %rd334;
	shr.u64 	%rd335, %rd511, 36;
	cvt.u32.u64 	%r281, %rd335;
	and.b32  	%r282, %r281, 15;
	and.b32  	%r283, %r280, 15;
	shr.u64 	%rd336, %rd511, 40;
	cvt.u32.u64 	%r284, %rd336;
	shr.u64 	%rd337, %rd511, 44;
	cvt.u32.u64 	%r285, %rd337;
	bfi.b32 	%r286, %r283, %r282, 8, 4;
	shl.b32 	%r287, %r285, 16;
	and.b32  	%r288, %r287, 983040;
	or.b32  	%r289, %r286, %r288;
	shl.b32 	%r290, %r284, 24;
	and.b32  	%r291, %r290, 251658240;
	or.b32  	%r17, %r289, %r291;
	shr.u64 	%rd338, %rd511, 48;
	cvt.u32.u64 	%r292, %rd338;
	shr.u64 	%rd339, %rd511, 52;
	cvt.u32.u64 	%r293, %rd339;
	and.b32  	%r294, %r293, 15;
	and.b32  	%r295, %r292, 15;
	shr.u64 	%rd340, %rd511, 56;
	cvt.u32.u64 	%r296, %rd340;
	bfi.b32 	%r297, %r295, %r294, 8, 4;
	and.b32  	%r298, %r285, 983040;
	or.b32  	%r299, %r297, %r298;
	shl.b32 	%r300, %r296, 24;
	and.b32  	%r301, %r300, 251658240;
	or.b32  	%r18, %r299, %r301;
	mov.u32 	%r149, 0;
	mov.u64 	%rd527, matrix;
	mov.u64 	%rd528, %rd2;
	mov.u32 	%r686, %r149;

$L__BB0_6:
	ld.const.u32 	%r303, [%rd527];
	// begin inline asm
	dp4a.u32.u32 %r302, %r303, %r3, %r149;
	// end inline asm
	ld.const.u32 	%r307, [%rd527+4];
	// begin inline asm
	dp4a.u32.u32 %r306, %r307, %r4, %r302;
	// end inline asm
	ld.const.u32 	%r311, [%rd527+8];
	// begin inline asm
	dp4a.u32.u32 %r310, %r311, %r5, %r306;
	// end inline asm
	ld.const.u32 	%r315, [%rd527+12];
	// begin inline asm
	dp4a.u32.u32 %r314, %r315, %r6, %r310;
	// end inline asm
	ld.const.u32 	%r319, [%rd527+16];
	// begin inline asm
	dp4a.u32.u32 %r318, %r319, %r7, %r314;
	// end inline asm
	ld.const.u32 	%r323, [%rd527+20];
	// begin inline asm
	dp4a.u32.u32 %r322, %r323, %r8, %r318;
	// end inline asm
	ld.const.u32 	%r327, [%rd527+24];
	// begin inline asm
	dp4a.u32.u32 %r326, %r327, %r9, %r322;
	// end inline asm
	ld.const.u32 	%r331, [%rd527+28];
	// begin inline asm
	dp4a.u32.u32 %r330, %r331, %r10, %r326;
	// end inline asm
	ld.const.u32 	%r335, [%rd527+32];
	// begin inline asm
	dp4a.u32.u32 %r334, %r335, %r11, %r330;
	// end inline asm
	ld.const.u32 	%r339, [%rd527+36];
	// begin inline asm
	dp4a.u32.u32 %r338, %r339, %r12, %r334;
	// end inline asm
	ld.const.u32 	%r343, [%rd527+40];
	// begin inline asm
	dp4a.u32.u32 %r342, %r343, %r13, %r338;
	// end inline asm
	ld.const.u32 	%r347, [%rd527+44];
	// begin inline asm
	dp4a.u32.u32 %r346, %r347, %r14, %r342;
	// end inline asm
	ld.const.u32 	%r351, [%rd527+48];
	// begin inline asm
	dp4a.u32.u32 %r350, %r351, %r15, %r346;
	// end inline asm
	ld.const.u32 	%r355, [%rd527+52];
	// begin inline asm
	dp4a.u32.u32 %r354, %r355, %r16, %r350;
	// end inline asm
	ld.const.u32 	%r359, [%rd527+56];
	// begin inline asm
	dp4a.u32.u32 %r358, %r359, %r17, %r354;
	// end inline asm
	ld.const.u32 	%r363, [%rd527+60];
	// begin inline asm
	dp4a.u32.u32 %r362, %r363, %r18, %r358;
	// end inline asm
	shr.u32 	%r566, %r362, 6;
	ld.const.u32 	%r367, [%rd527+64];
	// begin inline asm
	dp4a.u32.u32 %r366, %r367, %r3, %r149;
	// end inline asm
	ld.const.u32 	%r371, [%rd527+68];
	// begin inline asm
	dp4a.u32.u32 %r370, %r371, %r4, %r366;
	// end inline asm
	ld.const.u32 	%r375, [%rd527+72];
	// begin inline asm
	dp4a.u32.u32 %r374, %r375, %r5, %r370;
	// end inline asm
	ld.const.u32 	%r379, [%rd527+76];
	// begin inline asm
	dp4a.u32.u32 %r378, %r379, %r6, %r374;
	// end inline asm
	ld.const.u32 	%r383, [%rd527+80];
	// begin inline asm
	dp4a.u32.u32 %r382, %r383, %r7, %r378;
	// end inline asm
	ld.const.u32 	%r387, [%rd527+84];
	// begin inline asm
	dp4a.u32.u32 %r386, %r387, %r8, %r382;
	// end inline asm
	ld.const.u32 	%r391, [%rd527+88];
	// begin inline asm
	dp4a.u32.u32 %r390, %r391, %r9, %r386;
	// end inline asm
	ld.const.u32 	%r395, [%rd527+92];
	// begin inline asm
	dp4a.u32.u32 %r394, %r395, %r10, %r390;
	// end inline asm
	ld.const.u32 	%r399, [%rd527+96];
	// begin inline asm
	dp4a.u32.u32 %r398, %r399, %r11, %r394;
	// end inline asm
	ld.const.u32 	%r403, [%rd527+100];
	// begin inline asm
	dp4a.u32.u32 %r402, %r403, %r12, %r398;
	// end inline asm
	ld.const.u32 	%r407, [%rd527+104];
	// begin inline asm
	dp4a.u32.u32 %r406, %r407, %r13, %r402;
	// end inline asm
	ld.const.u32 	%r411, [%rd527+108];
	// begin inline asm
	dp4a.u32.u32 %r410, %r411, %r14, %r406;
	// end inline asm
	ld.const.u32 	%r415, [%rd527+112];
	// begin inline asm
	dp4a.u32.u32 %r414, %r415, %r15, %r410;
	// end inline asm
	ld.const.u32 	%r419, [%rd527+116];
	// begin inline asm
	dp4a.u32.u32 %r418, %r419, %r16, %r414;
	// end inline asm
	ld.const.u32 	%r423, [%rd527+120];
	// begin inline asm
	dp4a.u32.u32 %r422, %r423, %r17, %r418;
	// end inline asm
	ld.const.u32 	%r427, [%rd527+124];
	// begin inline asm
	dp4a.u32.u32 %r426, %r427, %r18, %r422;
	// end inline asm
	and.b32  	%r431, %r566, 240;
	shr.u32 	%r432, %r426, 10;
	ld.local.u8 	%r433, [%rd528];
	// begin inline asm
	lop3.b32 %r430, %r431, %r432, %r433, 0x56;
	// end inline asm
	st.local.u8 	[%rd528], %r430;
	ld.const.u32 	%r435, [%rd527+128];
	// begin inline asm
	dp4a.u32.u32 %r434, %r435, %r3, %r149;
	// end inline asm
	ld.const.u32 	%r439, [%rd527+132];
	// begin inline asm
	dp4a.u32.u32 %r438, %r439, %r4, %r434;
	// end inline asm
	ld.const.u32 	%r443, [%rd527+136];
	// begin inline asm
	dp4a.u32.u32 %r442, %r443, %r5, %r438;
	// end inline asm
	ld.const.u32 	%r447, [%rd527+140];
	// begin inline asm
	dp4a.u32.u32 %r446, %r447, %r6, %r442;
	// end inline asm
	ld.const.u32 	%r451, [%rd527+144];
	// begin inline asm
	dp4a.u32.u32 %r450, %r451, %r7, %r446;
	// end inline asm
	ld.const.u32 	%r455, [%rd527+148];
	// begin inline asm
	dp4a.u32.u32 %r454, %r455, %r8, %r450;
	// end inline asm
	ld.const.u32 	%r459, [%rd527+152];
	// begin inline asm
	dp4a.u32.u32 %r458, %r459, %r9, %r454;
	// end inline asm
	ld.const.u32 	%r463, [%rd527+156];
	// begin inline asm
	dp4a.u32.u32 %r462, %r463, %r10, %r458;
	// end inline asm
	ld.const.u32 	%r467, [%rd527+160];
	// begin inline asm
	dp4a.u32.u32 %r466, %r467, %r11, %r462;
	// end inline asm
	ld.const.u32 	%r471, [%rd527+164];
	// begin inline asm
	dp4a.u32.u32 %r470, %r471, %r12, %r466;
	// end inline asm
	ld.const.u32 	%r475, [%rd527+168];
	// begin inline asm
	dp4a.u32.u32 %r474, %r475, %r13, %r470;
	// end inline asm
	ld.const.u32 	%r479, [%rd527+172];
	// begin inline asm
	dp4a.u32.u32 %r478, %r479, %r14, %r474;
	// end inline asm
	ld.const.u32 	%r483, [%rd527+176];
	// begin inline asm
	dp4a.u32.u32 %r482, %r483, %r15, %r478;
	// end inline asm
	ld.const.u32 	%r487, [%rd527+180];
	// begin inline asm
	dp4a.u32.u32 %r486, %r487, %r16, %r482;
	// end inline asm
	ld.const.u32 	%r491, [%rd527+184];
	// begin inline asm
	dp4a.u32.u32 %r490, %r491, %r17, %r486;
	// end inline asm
	ld.const.u32 	%r495, [%rd527+188];
	// begin inline asm
	dp4a.u32.u32 %r494, %r495, %r18, %r490;
	// end inline asm
	shr.u32 	%r567, %r494, 6;
	ld.const.u32 	%r499, [%rd527+192];
	// begin inline asm
	dp4a.u32.u32 %r498, %r499, %r3, %r149;
	// end inline asm
	ld.const.u32 	%r503, [%rd527+196];
	// begin inline asm
	dp4a.u32.u32 %r502, %r503, %r4, %r498;
	// end inline asm
	ld.const.u32 	%r507, [%rd527+200];
	// begin inline asm
	dp4a.u32.u32 %r506, %r507, %r5, %r502;
	// end inline asm
	ld.const.u32 	%r511, [%rd527+204];
	// begin inline asm
	dp4a.u32.u32 %r510, %r511, %r6, %r506;
	// end inline asm
	ld.const.u32 	%r515, [%rd527+208];
	// begin inline asm
	dp4a.u32.u32 %r514, %r515, %r7, %r510;
	// end inline asm
	ld.const.u32 	%r519, [%rd527+212];
	// begin inline asm
	dp4a.u32.u32 %r518, %r519, %r8, %r514;
	// end inline asm
	ld.const.u32 	%r523, [%rd527+216];
	// begin inline asm
	dp4a.u32.u32 %r522, %r523, %r9, %r518;
	// end inline asm
	ld.const.u32 	%r527, [%rd527+220];
	// begin inline asm
	dp4a.u32.u32 %r526, %r527, %r10, %r522;
	// end inline asm
	ld.const.u32 	%r531, [%rd527+224];
	// begin inline asm
	dp4a.u32.u32 %r530, %r531, %r11, %r526;
	// end inline asm
	ld.const.u32 	%r535, [%rd527+228];
	// begin inline asm
	dp4a.u32.u32 %r534, %r535, %r12, %r530;
	// end inline asm
	ld.const.u32 	%r539, [%rd527+232];
	// begin inline asm
	dp4a.u32.u32 %r538, %r539, %r13, %r534;
	// end inline asm
	ld.const.u32 	%r543, [%rd527+236];
	// begin inline asm
	dp4a.u32.u32 %r542, %r543, %r14, %r538;
	// end inline asm
	ld.const.u32 	%r547, [%rd527+240];
	// begin inline asm
	dp4a.u32.u32 %r546, %r547, %r15, %r542;
	// end inline asm
	ld.const.u32 	%r551, [%rd527+244];
	// begin inline asm
	dp4a.u32.u32 %r550, %r551, %r16, %r546;
	// end inline asm
	ld.const.u32 	%r555, [%rd527+248];
	// begin inline asm
	dp4a.u32.u32 %r554, %r555, %r17, %r550;
	// end inline asm
	ld.const.u32 	%r559, [%rd527+252];
	// begin inline asm
	dp4a.u32.u32 %r558, %r559, %r18, %r554;
	// end inline asm
	and.b32  	%r563, %r567, 240;
	shr.u32 	%r564, %r558, 10;
	ld.local.u8 	%r565, [%rd528+1];
	// begin inline asm
	lop3.b32 %r562, %r563, %r564, %r565, 0x56;
	// end inline asm
	st.local.u8 	[%rd528+1], %r562;
	add.s64 	%rd528, %rd528, 2;
	add.s64 	%rd527, %rd527, 256;
	add.s32 	%r686, %r686, 2;
	setp.ne.s32 	%p9, %r686, 32;
	@%p9 bra 	$L__BB0_6;

	ld.local.u64 	%rd363, [%rd2];
	xor.b64  	%rd126, %rd363, 4239941492252378377;
	ld.local.u64 	%rd364, [%rd2+8];
	xor.b64  	%rd549, %rd364, 8746723911537738262;
	ld.local.u64 	%rd365, [%rd2+16];
	xor.b64  	%rd544, %rd365, 8796936657246353646;
	ld.local.u64 	%rd366, [%rd2+24];
	xor.b64  	%rd539, %rd366, 1272090201925444760;
	mov.u32 	%r687, 0;
	mov.u64 	%rd553, 8270816933120786537;
	mov.u64 	%rd552, -850687345431043546;
	mov.u64 	%rd551, 8596393687355028144;
	mov.u64 	%rd550, -4073852189716399785;
	mov.u64 	%rd548, -4539347866060507718;
	mov.u64 	%rd547, -3233781605604422593;
	mov.u64 	%rd546, 570094237299545110;
	mov.u64 	%rd545, 5171152063242093102;
	mov.u64 	%rd543, 6782861118970774626;
	mov.u64 	%rd542, 7812475424661425213;
	mov.u64 	%rd541, 9119540418498120711;
	mov.u64 	%rd540, -7873636174015165430;
	mov.u64 	%rd538, -9207053471590684088;
	mov.u64 	%rd537, 3370482334374859748;
	mov.u64 	%rd536, -1544774801229058759;
	mov.u64 	%rd535, 6096431547456407061;
	mov.u64 	%rd534, -1792185402154627366;
	mov.u64 	%rd533, -6864424130110145268;
	mov.u64 	%rd532, 5690099369266491460;
	mov.u64 	%rd531, -5074726839974049192;
	mov.u64 	%rd530, 1592359455985097269;
	mov.u64 	%rd529, RC;

$L__BB0_8:
	xor.b64  	%rd367, %rd553, %rd126;
	xor.b64  	%rd368, %rd367, %rd552;
	xor.b64  	%rd369, %rd368, %rd551;
	xor.b64  	%rd370, %rd369, %rd550;
	xor.b64  	%rd371, %rd548, %rd549;
	xor.b64  	%rd372, %rd371, %rd547;
	xor.b64  	%rd373, %rd372, %rd546;
	xor.b64  	%rd374, %rd373, %rd545;
	xor.b64  	%rd375, %rd543, %rd544;
	xor.b64  	%rd376, %rd375, %rd542;
	xor.b64  	%rd377, %rd376, %rd541;
	xor.b64  	%rd378, %rd377, %rd540;
	xor.b64  	%rd379, %rd538, %rd539;
	xor.b64  	%rd380, %rd379, %rd537;
	xor.b64  	%rd381, %rd380, %rd536;
	xor.b64  	%rd382, %rd381, %rd535;
	xor.b64  	%rd383, %rd533, %rd534;
	xor.b64  	%rd384, %rd383, %rd532;
	xor.b64  	%rd385, %rd384, %rd531;
	xor.b64  	%rd386, %rd385, %rd530;
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r569}, %rd374;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%r570,%dummy}, %rd374;
	}
	shf.l.wrap.b32 	%r571, %r570, %r569, 1;
	shf.l.wrap.b32 	%r572, %r569, %r570, 1;
	mov.b64 	%rd387, {%r572, %r571};
	xor.b64  	%rd388, %rd386, %rd387;
	xor.b64  	%rd389, %rd388, %rd126;
	xor.b64  	%rd390, %rd553, %rd388;
	xor.b64  	%rd391, %rd552, %rd388;
	xor.b64  	%rd392, %rd551, %rd388;
	xor.b64  	%rd393, %rd550, %rd388;
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r573}, %rd378;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%r574,%dummy}, %rd378;
	}
	shf.l.wrap.b32 	%r575, %r574, %r573, 1;
	shf.l.wrap.b32 	%r576, %r573, %r574, 1;
	mov.b64 	%rd394, {%r576, %r575};
	xor.b64  	%rd395, %rd394, %rd370;
	xor.b64  	%rd396, %rd549, %rd395;
	xor.b64  	%rd397, %rd548, %rd395;
	xor.b64  	%rd398, %rd547, %rd395;
	xor.b64  	%rd399, %rd546, %rd395;
	xor.b64  	%rd400, %rd545, %rd395;
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r577}, %rd382;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%r578,%dummy}, %rd382;
	}
	shf.l.wrap.b32 	%r579, %r578, %r577, 1;
	shf.l.wrap.b32 	%r580, %r577, %r578, 1;
	mov.b64 	%rd401, {%r580, %r579};
	xor.b64  	%rd402, %rd401, %rd374;
	xor.b64  	%rd403, %rd544, %rd402;
	xor.b64  	%rd404, %rd543, %rd402;
	xor.b64  	%rd405, %rd542, %rd402;
	xor.b64  	%rd406, %rd541, %rd402;
	xor.b64  	%rd407, %rd540, %rd402;
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r581}, %rd386;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%r582,%dummy}, %rd386;
	}
	shf.l.wrap.b32 	%r583, %r582, %r581, 1;
	shf.l.wrap.b32 	%r584, %r581, %r582, 1;
	mov.b64 	%rd408, {%r584, %r583};
	xor.b64  	%rd409, %rd408, %rd378;
	xor.b64  	%rd410, %rd539, %rd409;
	xor.b64  	%rd411, %rd538, %rd409;
	xor.b64  	%rd412, %rd537, %rd409;
	xor.b64  	%rd413, %rd536, %rd409;
	xor.b64  	%rd414, %rd535, %rd409;
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r585}, %rd370;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%r586,%dummy}, %rd370;
	}
	shf.l.wrap.b32 	%r587, %r586, %r585, 1;
	shf.l.wrap.b32 	%r588, %r585, %r586, 1;
	mov.b64 	%rd415, {%r588, %r587};
	xor.b64  	%rd416, %rd382, %rd415;
	xor.b64  	%rd417, %rd534, %rd416;
	xor.b64  	%rd418, %rd533, %rd416;
	xor.b64  	%rd419, %rd532, %rd416;
	xor.b64  	%rd420, %rd531, %rd416;
	xor.b64  	%rd421, %rd530, %rd416;
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r589}, %rd396;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%r590,%dummy}, %rd396;
	}
	shf.l.wrap.b32 	%r591, %r590, %r589, 1;
	shf.l.wrap.b32 	%r592, %r589, %r590, 1;
	mov.b64 	%rd422, {%r592, %r591};
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r593}, %rd391;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%r594,%dummy}, %rd391;
	}
	shf.l.wrap.b32 	%r595, %r594, %r593, 3;
	shf.l.wrap.b32 	%r596, %r593, %r594, 3;
	mov.b64 	%rd423, {%r596, %r595};
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r597}, %rd404;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%r598,%dummy}, %rd404;
	}
	shf.l.wrap.b32 	%r599, %r598, %r597, 6;
	shf.l.wrap.b32 	%r600, %r597, %r598, 6;
	mov.b64 	%rd424, {%r600, %r599};
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r601}, %rd398;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%r602,%dummy}, %rd398;
	}
	shf.l.wrap.b32 	%r603, %r602, %r601, 10;
	shf.l.wrap.b32 	%r604, %r601, %r602, 10;
	mov.b64 	%rd425, {%r604, %r603};
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r605}, %rd406;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%r606,%dummy}, %rd406;
	}
	shf.l.wrap.b32 	%r607, %r606, %r605, 15;
	shf.l.wrap.b32 	%r608, %r605, %r606, 15;
	mov.b64 	%rd426, {%r608, %r607};
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r609}, %rd413;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%r610,%dummy}, %rd413;
	}
	shf.l.wrap.b32 	%r611, %r610, %r609, 21;
	shf.l.wrap.b32 	%r612, %r609, %r610, 21;
	mov.b64 	%rd427, {%r612, %r611};
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r613}, %rd410;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%r614,%dummy}, %rd410;
	}
	shf.l.wrap.b32 	%r615, %r614, %r613, 28;
	shf.l.wrap.b32 	%r616, %r613, %r614, 28;
	mov.b64 	%rd428, {%r616, %r615};
	{
	.reg .b32 %dummy;
	mov.b64 	{%r617,%dummy}, %rd390;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r618}, %rd390;
	}
	shf.r.wrap.b32 	%r619, %r618, %r617, 28;
	shf.r.wrap.b32 	%r620, %r617, %r618, 28;
	mov.b64 	%rd429, {%r620, %r619};
	{
	.reg .b32 %dummy;
	mov.b64 	{%r621,%dummy}, %rd399;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r622}, %rd399;
	}
	shf.r.wrap.b32 	%r623, %r622, %r621, 19;
	shf.r.wrap.b32 	%r624, %r621, %r622, 19;
	mov.b64 	%rd430, {%r624, %r623};
	{
	.reg .b32 %dummy;
	mov.b64 	{%r625,%dummy}, %rd411;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r626}, %rd411;
	}
	shf.r.wrap.b32 	%r627, %r626, %r625, 9;
	shf.r.wrap.b32 	%r628, %r625, %r626, 9;
	mov.b64 	%rd431, {%r628, %r627};
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r629}, %rd400;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%r630,%dummy}, %rd400;
	}
	shf.l.wrap.b32 	%r631, %r630, %r629, 2;
	shf.l.wrap.b32 	%r632, %r629, %r630, 2;
	mov.b64 	%rd432, {%r632, %r631};
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r633}, %rd421;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%r634,%dummy}, %rd421;
	}
	shf.l.wrap.b32 	%r635, %r634, %r633, 14;
	shf.l.wrap.b32 	%r636, %r633, %r634, 14;
	mov.b64 	%rd433, {%r636, %r635};
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r637}, %rd417;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%r638,%dummy}, %rd417;
	}
	shf.l.wrap.b32 	%r639, %r638, %r637, 27;
	shf.l.wrap.b32 	%r640, %r637, %r638, 27;
	mov.b64 	%rd434, {%r640, %r639};
	{
	.reg .b32 %dummy;
	mov.b64 	{%r641,%dummy}, %rd392;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r642}, %rd392;
	}
	shf.r.wrap.b32 	%r643, %r642, %r641, 23;
	shf.r.wrap.b32 	%r644, %r641, %r642, 23;
	mov.b64 	%rd435, {%r644, %r643};
	{
	.reg .b32 %dummy;
	mov.b64 	{%r645,%dummy}, %rd414;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r646}, %rd414;
	}
	shf.r.wrap.b32 	%r647, %r646, %r645, 8;
	shf.r.wrap.b32 	%r648, %r645, %r646, 8;
	mov.b64 	%rd436, {%r648, %r647};
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r649}, %rd420;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%r650,%dummy}, %rd420;
	}
	shf.l.wrap.b32 	%r651, %r650, %r649, 8;
	shf.l.wrap.b32 	%r652, %r649, %r650, 8;
	mov.b64 	%rd437, {%r652, %r651};
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r653}, %rd412;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%r654,%dummy}, %rd412;
	}
	shf.l.wrap.b32 	%r655, %r654, %r653, 25;
	shf.l.wrap.b32 	%r656, %r653, %r654, 25;
	mov.b64 	%rd438, {%r656, %r655};
	{
	.reg .b32 %dummy;
	mov.b64 	{%r657,%dummy}, %rd405;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r658}, %rd405;
	}
	shf.r.wrap.b32 	%r659, %r658, %r657, 21;
	shf.r.wrap.b32 	%r660, %r657, %r658, 21;
	mov.b64 	%rd439, {%r660, %r659};
	{
	.reg .b32 %dummy;
	mov.b64 	{%r661,%dummy}, %rd403;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r662}, %rd403;
	}
	shf.r.wrap.b32 	%r663, %r662, %r661, 2;
	shf.r.wrap.b32 	%r664, %r661, %r662, 2;
	mov.b64 	%rd440, {%r664, %r663};
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r665}, %rd393;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%r666,%dummy}, %rd393;
	}
	shf.l.wrap.b32 	%r667, %r666, %r665, 18;
	shf.l.wrap.b32 	%r668, %r665, %r666, 18;
	mov.b64 	%rd441, {%r668, %r667};
	{
	.reg .b32 %dummy;
	mov.b64 	{%r669,%dummy}, %rd419;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r670}, %rd419;
	}
	shf.r.wrap.b32 	%r671, %r670, %r669, 25;
	shf.r.wrap.b32 	%r672, %r669, %r670, 25;
	mov.b64 	%rd442, {%r672, %r671};
	{
	.reg .b32 %dummy;
	mov.b64 	{%r673,%dummy}, %rd407;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r674}, %rd407;
	}
	shf.r.wrap.b32 	%r675, %r674, %r673, 3;
	shf.r.wrap.b32 	%r676, %r673, %r674, 3;
	mov.b64 	%rd443, {%r676, %r675};
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r677}, %rd418;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%r678,%dummy}, %rd418;
	}
	shf.l.wrap.b32 	%r679, %r678, %r677, 20;
	shf.l.wrap.b32 	%r680, %r677, %r678, 20;
	mov.b64 	%rd444, {%r680, %r679};
	{
	.reg .b32 %dummy;
	mov.b64 	{%r681,%dummy}, %rd397;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r682}, %rd397;
	}
	shf.r.wrap.b32 	%r683, %r682, %r681, 20;
	shf.r.wrap.b32 	%r684, %r681, %r682, 20;
	mov.b64 	%rd445, {%r684, %r683};
	not.b64 	%rd446, %rd445;
	and.b64  	%rd447, %rd439, %rd446;
	xor.b64  	%rd448, %rd447, %rd389;
	not.b64 	%rd449, %rd439;
	and.b64  	%rd450, %rd427, %rd449;
	xor.b64  	%rd549, %rd450, %rd445;
	not.b64 	%rd451, %rd427;
	and.b64  	%rd452, %rd433, %rd451;
	xor.b64  	%rd544, %rd452, %rd439;
	not.b64 	%rd453, %rd433;
	and.b64  	%rd454, %rd389, %rd453;
	xor.b64  	%rd539, %rd454, %rd427;
	not.b64 	%rd455, %rd389;
	and.b64  	%rd456, %rd445, %rd455;
	xor.b64  	%rd534, %rd433, %rd456;
	not.b64 	%rd457, %rd444;
	and.b64  	%rd458, %rd423, %rd457;
	xor.b64  	%rd553, %rd458, %rd428;
	not.b64 	%rd459, %rd423;
	and.b64  	%rd460, %rd430, %rd459;
	xor.b64  	%rd548, %rd460, %rd444;
	not.b64 	%rd461, %rd430;
	and.b64  	%rd462, %rd443, %rd461;
	xor.b64  	%rd543, %rd462, %rd423;
	not.b64 	%rd463, %rd443;
	and.b64  	%rd464, %rd428, %rd463;
	xor.b64  	%rd538, %rd464, %rd430;
	not.b64 	%rd465, %rd428;
	and.b64  	%rd466, %rd444, %rd465;
	xor.b64  	%rd533, %rd443, %rd466;
	not.b64 	%rd467, %rd424;
	and.b64  	%rd468, %rd438, %rd467;
	xor.b64  	%rd552, %rd468, %rd422;
	not.b64 	%rd469, %rd438;
	and.b64  	%rd470, %rd437, %rd469;
	xor.b64  	%rd547, %rd470, %rd424;
	not.b64 	%rd471, %rd437;
	and.b64  	%rd472, %rd441, %rd471;
	xor.b64  	%rd542, %rd472, %rd438;
	not.b64 	%rd473, %rd441;
	and.b64  	%rd474, %rd422, %rd473;
	xor.b64  	%rd537, %rd474, %rd437;
	not.b64 	%rd475, %rd422;
	and.b64  	%rd476, %rd424, %rd475;
	xor.b64  	%rd532, %rd441, %rd476;
	not.b64 	%rd477, %rd429;
	and.b64  	%rd478, %rd425, %rd477;
	xor.b64  	%rd551, %rd478, %rd434;
	not.b64 	%rd479, %rd425;
	and.b64  	%rd480, %rd426, %rd479;
	xor.b64  	%rd546, %rd480, %rd429;
	not.b64 	%rd481, %rd426;
	and.b64  	%rd482, %rd436, %rd481;
	xor.b64  	%rd541, %rd482, %rd425;
	not.b64 	%rd483, %rd436;
	and.b64  	%rd484, %rd434, %rd483;
	xor.b64  	%rd536, %rd484, %rd426;
	not.b64 	%rd485, %rd434;
	and.b64  	%rd486, %rd429, %rd485;
	xor.b64  	%rd531, %rd436, %rd486;
	not.b64 	%rd487, %rd431;
	and.b64  	%rd488, %rd442, %rd487;
	xor.b64  	%rd550, %rd488, %rd440;
	not.b64 	%rd489, %rd442;
	and.b64  	%rd490, %rd435, %rd489;
	xor.b64  	%rd545, %rd490, %rd431;
	not.b64 	%rd491, %rd435;
	and.b64  	%rd492, %rd432, %rd491;
	xor.b64  	%rd540, %rd492, %rd442;
	not.b64 	%rd493, %rd432;
	and.b64  	%rd494, %rd440, %rd493;
	xor.b64  	%rd535, %rd494, %rd435;
	not.b64 	%rd495, %rd440;
	and.b64  	%rd496, %rd431, %rd495;
	xor.b64  	%rd530, %rd432, %rd496;
	ld.global.nc.u64 	%rd497, [%rd529];
	xor.b64  	%rd126, %rd448, %rd497;
	add.s64 	%rd529, %rd529, 8;
	add.s32 	%r687, %r687, 1;
	setp.ne.s32 	%p10, %r687, 24;
	@%p10 bra 	$L__BB0_8;

	st.local.u64 	[%rd2], %rd126;
	st.local.u64 	[%rd2+8], %rd549;
	st.local.u64 	[%rd2+16], %rd544;
	st.local.u64 	[%rd2+24], %rd539;
	ld.const.u64 	%rd128, [target+24];
	setp.eq.s64 	%p11, %rd539, %rd128;
	@%p11 bra 	$L__BB0_11;
	bra.uni 	$L__BB0_10;

$L__BB0_11:
	ld.const.u64 	%rd129, [target+16];
	setp.eq.s64 	%p12, %rd544, %rd129;
	@%p12 bra 	$L__BB0_13;
	bra.uni 	$L__BB0_12;

$L__BB0_13:
	ld.const.u64 	%rd130, [target+8];
	setp.eq.s64 	%p13, %rd549, %rd130;
	@%p13 bra 	$L__BB0_15;
	bra.uni 	$L__BB0_14;

$L__BB0_15:
	ld.const.u64 	%rd498, [target];
	setp.lt.u64 	%p15, %rd126, %rd498;
	bra.uni 	$L__BB0_16;

$L__BB0_10:
	setp.lt.u64 	%p15, %rd539, %rd128;
	bra.uni 	$L__BB0_16;

$L__BB0_12:
	setp.lt.u64 	%p15, %rd544, %rd129;
	bra.uni 	$L__BB0_16;

$L__BB0_14:
	setp.lt.u64 	%p15, %rd549, %rd130;

$L__BB0_16:
	not.pred 	%p14, %p15;
	@%p14 bra 	$L__BB0_18;

	mov.u64 	%rd499, 0;
	atom.global.cas.b64 	%rd500, [%rd1], %rd499, %rd4;

$L__BB0_18:
	ret;

}

