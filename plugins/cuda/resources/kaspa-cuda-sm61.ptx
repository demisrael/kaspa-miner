//
// Generated by NVIDIA NVVM Compiler
//
// Compiler Build ID: CL-29618528
// Cuda compilation tools, release 11.2, V11.2.152
// Based on NVVM 7.0.1
//

.version 7.2
.target sm_61
.address_size 64

	// .globl	heavy_hash
.global .align 1 .b8 rho[24] = {1, 3, 6, 10, 15, 21, 28, 36, 45, 55, 2, 14, 27, 41, 56, 8, 25, 43, 62, 18, 39, 61, 20, 44};
.global .align 1 .b8 pi[24] = {10, 7, 11, 17, 18, 3, 5, 16, 8, 21, 24, 4, 15, 23, 19, 13, 12, 2, 20, 14, 22, 9, 6, 1};
.global .align 8 .b8 RC[192] = {1, 0, 0, 0, 0, 0, 0, 0, 130, 128, 0, 0, 0, 0, 0, 0, 138, 128, 0, 0, 0, 0, 0, 128, 0, 128, 0, 128, 0, 0, 0, 128, 139, 128, 0, 0, 0, 0, 0, 0, 1, 0, 0, 128, 0, 0, 0, 0, 129, 128, 0, 128, 0, 0, 0, 128, 9, 128, 0, 0, 0, 0, 0, 128, 138, 0, 0, 0, 0, 0, 0, 0, 136, 0, 0, 0, 0, 0, 0, 0, 9, 128, 0, 128, 0, 0, 0, 0, 10, 0, 0, 128, 0, 0, 0, 0, 139, 128, 0, 128, 0, 0, 0, 0, 139, 0, 0, 0, 0, 0, 0, 128, 137, 128, 0, 0, 0, 0, 0, 128, 3, 128, 0, 0, 0, 0, 0, 128, 2, 128, 0, 0, 0, 0, 0, 128, 128, 0, 0, 0, 0, 0, 0, 128, 10, 128, 0, 0, 0, 0, 0, 0, 10, 0, 0, 128, 0, 0, 0, 128, 129, 128, 0, 128, 0, 0, 0, 128, 128, 128, 0, 0, 0, 0, 0, 128, 1, 0, 0, 128, 0, 0, 0, 0, 8, 128, 0, 128, 0, 0, 0, 128};
.global .align 8 .b8 _ZZ15xoshiro256_jumpP10ulonglong4E4JUMP[32] = {186, 10, 253, 60, 211, 198, 14, 24, 44, 57, 201, 240, 102, 18, 166, 213, 170, 201, 63, 224, 24, 38, 88, 169, 28, 102, 177, 41, 69, 220, 171, 57};
.global .align 8 .b8 _ZZ20xoshiro256_long_jumpP10ulonglong4E9LONG_JUMP[32] = {191, 203, 253, 254, 62, 93, 225, 118, 179, 47, 82, 28, 68, 78, 0, 197, 65, 226, 78, 133, 105, 0, 113, 119, 53, 230, 203, 42, 176, 155, 16, 57};
.const .align 1 .b8 matrix[4096];
.const .align 8 .b8 hash_header[72];
.const .align 8 .b8 target[32];
.const .align 1 .b8 powP[200] = {61, 216, 246, 161, 13, 255, 60, 17, 60, 126, 2, 183, 85, 136, 191, 41, 210, 68, 251, 14, 114, 46, 95, 30, 160, 105, 152, 245, 163, 164, 165, 27, 101, 45, 94, 135, 202, 175, 47, 123, 70, 226, 220, 41, 214, 97, 239, 74, 16, 91, 65, 173, 30, 152, 58, 24, 156, 194, 155, 120, 12, 246, 107, 119, 64, 49, 102, 136, 51, 241, 235, 248, 240, 95, 40, 67, 60, 28, 101, 46, 10, 74, 241, 64, 5, 7, 150, 15, 82, 145, 41, 91, 135, 103, 227, 68, 21, 55, 177, 37, 164, 241, 112, 236, 137, 218, 233, 130, 143, 93, 200, 230, 35, 178, 180, 133, 31, 96, 26, 178, 70, 106, 163, 100, 144, 84, 133, 52, 26, 133, 47, 122, 28, 221, 6, 15, 66, 177, 59, 86, 29, 2, 162, 193, 228, 104, 22, 69, 228, 229, 29, 186, 141, 95, 9, 5, 65, 87, 2, 209, 74, 207, 206, 155, 132, 78, 202, 137, 219, 46, 116, 168, 39, 148, 176, 72, 114, 82, 139, 231, 156, 206, 252, 177, 188, 165, 175, 130, 207, 41, 17, 93, 131, 67, 130, 111, 120, 124, 185, 2};
.const .align 1 .b8 heavyP[200] = {9, 133, 36, 178, 82, 76, 215, 58, 22, 66, 159, 47, 14, 155, 98, 121, 238, 248, 199, 22, 72, 255, 20, 122, 152, 100, 5, 128, 76, 95, 167, 17, 218, 206, 238, 68, 223, 224, 32, 231, 105, 64, 243, 20, 46, 216, 199, 114, 186, 53, 137, 147, 42, 255, 0, 193, 98, 196, 15, 37, 64, 144, 33, 94, 72, 106, 207, 13, 166, 249, 57, 128, 12, 61, 42, 121, 159, 170, 188, 160, 38, 162, 169, 208, 93, 192, 49, 244, 63, 140, 193, 84, 195, 76, 31, 211, 61, 204, 105, 167, 1, 125, 107, 108, 228, 147, 36, 86, 211, 91, 198, 46, 68, 176, 205, 153, 58, 75, 247, 78, 176, 242, 52, 84, 131, 134, 76, 119, 22, 148, 188, 54, 176, 97, 233, 7, 7, 204, 101, 119, 177, 29, 143, 126, 57, 109, 196, 186, 128, 219, 143, 234, 88, 202, 52, 123, 211, 242, 146, 185, 87, 185, 129, 132, 4, 197, 118, 199, 46, 194, 18, 81, 103, 159, 195, 71, 10, 12, 41, 181, 157, 57, 187, 146, 21, 198, 159, 47, 49, 224, 154, 84, 53, 218, 185, 16, 125, 50, 25, 22};

.visible .entry heavy_hash(
	.param .u64 heavy_hash_param_0,
	.param .u64 heavy_hash_param_1,
	.param .u64 heavy_hash_param_2
)
{
	.local .align 8 .b8 	__local_depot0[32];
	.reg .b64 	%SP;
	.reg .b64 	%SPL;
	.reg .pred 	%p<16>;
	.reg .b16 	%rs<13>;
	.reg .b32 	%r<555>;
	.reg .b64 	%rd<555>;


	mov.u64 	%SPL, __local_depot0;
	ld.param.u64 	%rd132, [heavy_hash_param_0];
	ld.param.u64 	%rd131, [heavy_hash_param_1];
	ld.param.u64 	%rd133, [heavy_hash_param_2];
	cvta.to.global.u64 	%rd1, %rd133;
	mov.u32 	%r23, %ntid.x;
	mov.u32 	%r24, %ctaid.x;
	mov.u32 	%r25, %tid.x;
	mad.lo.s32 	%r26, %r24, %r23, %r25;
	cvt.s64.s32 	%rd2, %r26;
	setp.ge.u64 	%p6, %rd2, %rd132;
	@%p6 bra 	LBB0_18;

	cvt.u32.u64 	%r27, %rd2;
	setp.ne.s32 	%p7, %r27, 0;
	@%p7 bra 	LBB0_3;

	mov.u64 	%rd134, 0;
	st.global.u64 	[%rd1], %rd134;

LBB0_3:
	cvta.to.global.u64 	%rd151, %rd131;
	shl.b64 	%rd152, %rd2, 5;
	add.s64 	%rd153, %rd151, %rd152;
	ld.global.v2.u64 	{%rd154, %rd155}, [%rd153];
	mov.u32 	%r552, 0;
	mul.lo.s64 	%rd158, %rd155, 5;
	{
	.reg .b64 %lhs;
	.reg .b64 %rhs;
	shl.b64 	%lhs, %rd158, 7;
	shr.b64 	%rhs, %rd158, 57;
	add.u64 	%rd159, %lhs, %rhs;
	}
	mul.lo.s64 	%rd3, %rd159, 9;
	shl.b64 	%rd160, %rd155, 17;
	ld.global.v2.u64 	{%rd161, %rd162}, [%rd153+16];
	xor.b64  	%rd165, %rd161, %rd154;
	xor.b64  	%rd166, %rd162, %rd155;
	xor.b64  	%rd167, %rd155, %rd165;
	xor.b64  	%rd168, %rd154, %rd166;
	st.global.v2.u64 	[%rd153], {%rd168, %rd167};
	{
	.reg .b32 %dummy;
	mov.b64 	{%r29,%dummy}, %rd166;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r30}, %rd166;
	}
	shf.r.wrap.b32 	%r31, %r30, %r29, 19;
	shf.r.wrap.b32 	%r32, %r29, %r30, 19;
	mov.b64 	%rd169, {%r32, %r31};
	xor.b64  	%rd170, %rd165, %rd160;
	st.global.v2.u64 	[%rd153+16], {%rd170, %rd169};
	ld.const.u64 	%rd171, [hash_header];
	xor.b64  	%rd526, %rd171, 1242148031264380989;
	ld.const.u64 	%rd172, [hash_header+8];
	xor.b64  	%rd521, %rd172, 3008272977830772284;
	ld.const.u64 	%rd173, [hash_header+16];
	xor.b64  	%rd516, %rd173, 2188519011337848018;
	ld.const.u64 	%rd174, [hash_header+24];
	xor.b64  	%rd511, %rd174, 1992179434288343456;
	ld.const.u64 	%rd175, [hash_header+32];
	xor.b64  	%rd506, %rd175, 8876506674959887717;
	ld.const.u64 	%rd176, [hash_header+40];
	xor.b64  	%rd525, %rd176, 5399642050693751366;
	ld.const.u64 	%rd177, [hash_header+48];
	xor.b64  	%rd520, %rd177, 1745875063082670864;
	ld.const.u64 	%rd178, [hash_header+56];
	xor.b64  	%rd515, %rd178, 8605242046444978844;
	ld.const.u64 	%rd179, [hash_header+64];
	xor.b64  	%rd510, %rd179, -510048929142394560;
	xor.b64  	%rd505, %rd3, 3343109343542796272;
	mov.u64 	%rd524, 1123092876221303306;
	mov.u64 	%rd523, 3784524041015224902;
	mov.u64 	%rd522, -8517909413761200310;
	mov.u64 	%rd519, 4963925045340115282;
	mov.u64 	%rd518, 1082795874807940378;
	mov.u64 	%rd517, 5237849264682708699;
	mov.u64 	%rd514, -1409360996057663723;
	mov.u64 	%rd513, -4494027153138273982;
	mov.u64 	%rd512, -5621391061570334094;
	mov.u64 	%rd509, -1817099578685924727;
	mov.u64 	%rd508, -5035616039755945756;
	mov.u64 	%rd507, 6706187291358897596;
	mov.u64 	%rd504, -5613068297060437469;
	mov.u64 	%rd503, -3386048033060200563;
	mov.u64 	%rd502, 196324915476054915;
	mov.u64 	%rd501, RC;

LBB0_4:
	xor.b64  	%rd180, %rd525, %rd526;
	xor.b64  	%rd181, %rd180, %rd524;
	xor.b64  	%rd182, %rd181, %rd523;
	xor.b64  	%rd183, %rd182, %rd522;
	xor.b64  	%rd184, %rd520, %rd521;
	xor.b64  	%rd185, %rd184, %rd519;
	xor.b64  	%rd186, %rd185, %rd518;
	xor.b64  	%rd187, %rd186, %rd517;
	xor.b64  	%rd188, %rd515, %rd516;
	xor.b64  	%rd189, %rd188, %rd514;
	xor.b64  	%rd190, %rd189, %rd513;
	xor.b64  	%rd191, %rd190, %rd512;
	xor.b64  	%rd192, %rd510, %rd511;
	xor.b64  	%rd193, %rd192, %rd509;
	xor.b64  	%rd194, %rd193, %rd508;
	xor.b64  	%rd195, %rd194, %rd507;
	xor.b64  	%rd196, %rd505, %rd506;
	xor.b64  	%rd197, %rd196, %rd504;
	xor.b64  	%rd198, %rd197, %rd503;
	xor.b64  	%rd199, %rd198, %rd502;
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r33}, %rd187;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%r34,%dummy}, %rd187;
	}
	shf.l.wrap.b32 	%r35, %r34, %r33, 1;
	shf.l.wrap.b32 	%r36, %r33, %r34, 1;
	mov.b64 	%rd200, {%r36, %r35};
	xor.b64  	%rd201, %rd199, %rd200;
	xor.b64  	%rd202, %rd201, %rd526;
	xor.b64  	%rd203, %rd525, %rd201;
	xor.b64  	%rd204, %rd524, %rd201;
	xor.b64  	%rd205, %rd523, %rd201;
	xor.b64  	%rd206, %rd522, %rd201;
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r37}, %rd191;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%r38,%dummy}, %rd191;
	}
	shf.l.wrap.b32 	%r39, %r38, %r37, 1;
	shf.l.wrap.b32 	%r40, %r37, %r38, 1;
	mov.b64 	%rd207, {%r40, %r39};
	xor.b64  	%rd208, %rd207, %rd183;
	xor.b64  	%rd209, %rd521, %rd208;
	xor.b64  	%rd210, %rd520, %rd208;
	xor.b64  	%rd211, %rd519, %rd208;
	xor.b64  	%rd212, %rd518, %rd208;
	xor.b64  	%rd213, %rd517, %rd208;
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r41}, %rd195;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%r42,%dummy}, %rd195;
	}
	shf.l.wrap.b32 	%r43, %r42, %r41, 1;
	shf.l.wrap.b32 	%r44, %r41, %r42, 1;
	mov.b64 	%rd214, {%r44, %r43};
	xor.b64  	%rd215, %rd214, %rd187;
	xor.b64  	%rd216, %rd516, %rd215;
	xor.b64  	%rd217, %rd515, %rd215;
	xor.b64  	%rd218, %rd514, %rd215;
	xor.b64  	%rd219, %rd513, %rd215;
	xor.b64  	%rd220, %rd512, %rd215;
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r45}, %rd199;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%r46,%dummy}, %rd199;
	}
	shf.l.wrap.b32 	%r47, %r46, %r45, 1;
	shf.l.wrap.b32 	%r48, %r45, %r46, 1;
	mov.b64 	%rd221, {%r48, %r47};
	xor.b64  	%rd222, %rd221, %rd191;
	xor.b64  	%rd223, %rd511, %rd222;
	xor.b64  	%rd224, %rd510, %rd222;
	xor.b64  	%rd225, %rd509, %rd222;
	xor.b64  	%rd226, %rd508, %rd222;
	xor.b64  	%rd227, %rd507, %rd222;
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r49}, %rd183;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%r50,%dummy}, %rd183;
	}
	shf.l.wrap.b32 	%r51, %r50, %r49, 1;
	shf.l.wrap.b32 	%r52, %r49, %r50, 1;
	mov.b64 	%rd228, {%r52, %r51};
	xor.b64  	%rd229, %rd195, %rd228;
	xor.b64  	%rd230, %rd506, %rd229;
	xor.b64  	%rd231, %rd505, %rd229;
	xor.b64  	%rd232, %rd504, %rd229;
	xor.b64  	%rd233, %rd503, %rd229;
	xor.b64  	%rd234, %rd502, %rd229;
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r53}, %rd209;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%r54,%dummy}, %rd209;
	}
	shf.l.wrap.b32 	%r55, %r54, %r53, 1;
	shf.l.wrap.b32 	%r56, %r53, %r54, 1;
	mov.b64 	%rd235, {%r56, %r55};
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r57}, %rd204;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%r58,%dummy}, %rd204;
	}
	shf.l.wrap.b32 	%r59, %r58, %r57, 3;
	shf.l.wrap.b32 	%r60, %r57, %r58, 3;
	mov.b64 	%rd236, {%r60, %r59};
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r61}, %rd217;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%r62,%dummy}, %rd217;
	}
	shf.l.wrap.b32 	%r63, %r62, %r61, 6;
	shf.l.wrap.b32 	%r64, %r61, %r62, 6;
	mov.b64 	%rd237, {%r64, %r63};
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r65}, %rd211;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%r66,%dummy}, %rd211;
	}
	shf.l.wrap.b32 	%r67, %r66, %r65, 10;
	shf.l.wrap.b32 	%r68, %r65, %r66, 10;
	mov.b64 	%rd238, {%r68, %r67};
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r69}, %rd219;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%r70,%dummy}, %rd219;
	}
	shf.l.wrap.b32 	%r71, %r70, %r69, 15;
	shf.l.wrap.b32 	%r72, %r69, %r70, 15;
	mov.b64 	%rd239, {%r72, %r71};
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r73}, %rd226;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%r74,%dummy}, %rd226;
	}
	shf.l.wrap.b32 	%r75, %r74, %r73, 21;
	shf.l.wrap.b32 	%r76, %r73, %r74, 21;
	mov.b64 	%rd240, {%r76, %r75};
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r77}, %rd223;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%r78,%dummy}, %rd223;
	}
	shf.l.wrap.b32 	%r79, %r78, %r77, 28;
	shf.l.wrap.b32 	%r80, %r77, %r78, 28;
	mov.b64 	%rd241, {%r80, %r79};
	{
	.reg .b32 %dummy;
	mov.b64 	{%r81,%dummy}, %rd203;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r82}, %rd203;
	}
	shf.r.wrap.b32 	%r83, %r82, %r81, 28;
	shf.r.wrap.b32 	%r84, %r81, %r82, 28;
	mov.b64 	%rd242, {%r84, %r83};
	{
	.reg .b32 %dummy;
	mov.b64 	{%r85,%dummy}, %rd212;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r86}, %rd212;
	}
	shf.r.wrap.b32 	%r87, %r86, %r85, 19;
	shf.r.wrap.b32 	%r88, %r85, %r86, 19;
	mov.b64 	%rd243, {%r88, %r87};
	{
	.reg .b32 %dummy;
	mov.b64 	{%r89,%dummy}, %rd224;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r90}, %rd224;
	}
	shf.r.wrap.b32 	%r91, %r90, %r89, 9;
	shf.r.wrap.b32 	%r92, %r89, %r90, 9;
	mov.b64 	%rd244, {%r92, %r91};
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r93}, %rd213;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%r94,%dummy}, %rd213;
	}
	shf.l.wrap.b32 	%r95, %r94, %r93, 2;
	shf.l.wrap.b32 	%r96, %r93, %r94, 2;
	mov.b64 	%rd245, {%r96, %r95};
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r97}, %rd234;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%r98,%dummy}, %rd234;
	}
	shf.l.wrap.b32 	%r99, %r98, %r97, 14;
	shf.l.wrap.b32 	%r100, %r97, %r98, 14;
	mov.b64 	%rd246, {%r100, %r99};
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r101}, %rd230;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%r102,%dummy}, %rd230;
	}
	shf.l.wrap.b32 	%r103, %r102, %r101, 27;
	shf.l.wrap.b32 	%r104, %r101, %r102, 27;
	mov.b64 	%rd247, {%r104, %r103};
	{
	.reg .b32 %dummy;
	mov.b64 	{%r105,%dummy}, %rd205;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r106}, %rd205;
	}
	shf.r.wrap.b32 	%r107, %r106, %r105, 23;
	shf.r.wrap.b32 	%r108, %r105, %r106, 23;
	mov.b64 	%rd248, {%r108, %r107};
	{
	.reg .b32 %dummy;
	mov.b64 	{%r109,%dummy}, %rd227;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r110}, %rd227;
	}
	shf.r.wrap.b32 	%r111, %r110, %r109, 8;
	shf.r.wrap.b32 	%r112, %r109, %r110, 8;
	mov.b64 	%rd249, {%r112, %r111};
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r113}, %rd233;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%r114,%dummy}, %rd233;
	}
	shf.l.wrap.b32 	%r115, %r114, %r113, 8;
	shf.l.wrap.b32 	%r116, %r113, %r114, 8;
	mov.b64 	%rd250, {%r116, %r115};
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r117}, %rd225;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%r118,%dummy}, %rd225;
	}
	shf.l.wrap.b32 	%r119, %r118, %r117, 25;
	shf.l.wrap.b32 	%r120, %r117, %r118, 25;
	mov.b64 	%rd251, {%r120, %r119};
	{
	.reg .b32 %dummy;
	mov.b64 	{%r121,%dummy}, %rd218;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r122}, %rd218;
	}
	shf.r.wrap.b32 	%r123, %r122, %r121, 21;
	shf.r.wrap.b32 	%r124, %r121, %r122, 21;
	mov.b64 	%rd252, {%r124, %r123};
	{
	.reg .b32 %dummy;
	mov.b64 	{%r125,%dummy}, %rd216;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r126}, %rd216;
	}
	shf.r.wrap.b32 	%r127, %r126, %r125, 2;
	shf.r.wrap.b32 	%r128, %r125, %r126, 2;
	mov.b64 	%rd253, {%r128, %r127};
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r129}, %rd206;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%r130,%dummy}, %rd206;
	}
	shf.l.wrap.b32 	%r131, %r130, %r129, 18;
	shf.l.wrap.b32 	%r132, %r129, %r130, 18;
	mov.b64 	%rd254, {%r132, %r131};
	{
	.reg .b32 %dummy;
	mov.b64 	{%r133,%dummy}, %rd232;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r134}, %rd232;
	}
	shf.r.wrap.b32 	%r135, %r134, %r133, 25;
	shf.r.wrap.b32 	%r136, %r133, %r134, 25;
	mov.b64 	%rd255, {%r136, %r135};
	{
	.reg .b32 %dummy;
	mov.b64 	{%r137,%dummy}, %rd220;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r138}, %rd220;
	}
	shf.r.wrap.b32 	%r139, %r138, %r137, 3;
	shf.r.wrap.b32 	%r140, %r137, %r138, 3;
	mov.b64 	%rd256, {%r140, %r139};
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r141}, %rd231;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%r142,%dummy}, %rd231;
	}
	shf.l.wrap.b32 	%r143, %r142, %r141, 20;
	shf.l.wrap.b32 	%r144, %r141, %r142, 20;
	mov.b64 	%rd257, {%r144, %r143};
	{
	.reg .b32 %dummy;
	mov.b64 	{%r145,%dummy}, %rd210;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r146}, %rd210;
	}
	shf.r.wrap.b32 	%r147, %r146, %r145, 20;
	shf.r.wrap.b32 	%r148, %r145, %r146, 20;
	mov.b64 	%rd258, {%r148, %r147};
	not.b64 	%rd259, %rd258;
	and.b64  	%rd260, %rd252, %rd259;
	xor.b64  	%rd261, %rd260, %rd202;
	not.b64 	%rd262, %rd252;
	and.b64  	%rd263, %rd240, %rd262;
	xor.b64  	%rd521, %rd263, %rd258;
	not.b64 	%rd264, %rd240;
	and.b64  	%rd265, %rd246, %rd264;
	xor.b64  	%rd516, %rd265, %rd252;
	not.b64 	%rd266, %rd246;
	and.b64  	%rd267, %rd202, %rd266;
	xor.b64  	%rd511, %rd267, %rd240;
	not.b64 	%rd268, %rd202;
	and.b64  	%rd269, %rd258, %rd268;
	xor.b64  	%rd506, %rd246, %rd269;
	not.b64 	%rd270, %rd257;
	and.b64  	%rd271, %rd236, %rd270;
	xor.b64  	%rd525, %rd271, %rd241;
	not.b64 	%rd272, %rd236;
	and.b64  	%rd273, %rd243, %rd272;
	xor.b64  	%rd520, %rd273, %rd257;
	not.b64 	%rd274, %rd243;
	and.b64  	%rd275, %rd256, %rd274;
	xor.b64  	%rd515, %rd275, %rd236;
	not.b64 	%rd276, %rd256;
	and.b64  	%rd277, %rd241, %rd276;
	xor.b64  	%rd510, %rd277, %rd243;
	not.b64 	%rd278, %rd241;
	and.b64  	%rd279, %rd257, %rd278;
	xor.b64  	%rd505, %rd256, %rd279;
	not.b64 	%rd280, %rd237;
	and.b64  	%rd281, %rd251, %rd280;
	xor.b64  	%rd524, %rd281, %rd235;
	not.b64 	%rd282, %rd251;
	and.b64  	%rd283, %rd250, %rd282;
	xor.b64  	%rd519, %rd283, %rd237;
	not.b64 	%rd284, %rd250;
	and.b64  	%rd285, %rd254, %rd284;
	xor.b64  	%rd514, %rd285, %rd251;
	not.b64 	%rd286, %rd254;
	and.b64  	%rd287, %rd235, %rd286;
	xor.b64  	%rd509, %rd287, %rd250;
	not.b64 	%rd288, %rd235;
	and.b64  	%rd289, %rd237, %rd288;
	xor.b64  	%rd504, %rd254, %rd289;
	not.b64 	%rd290, %rd242;
	and.b64  	%rd291, %rd238, %rd290;
	xor.b64  	%rd523, %rd291, %rd247;
	not.b64 	%rd292, %rd238;
	and.b64  	%rd293, %rd239, %rd292;
	xor.b64  	%rd518, %rd293, %rd242;
	not.b64 	%rd294, %rd239;
	and.b64  	%rd295, %rd249, %rd294;
	xor.b64  	%rd513, %rd295, %rd238;
	not.b64 	%rd296, %rd249;
	and.b64  	%rd297, %rd247, %rd296;
	xor.b64  	%rd508, %rd297, %rd239;
	not.b64 	%rd298, %rd247;
	and.b64  	%rd299, %rd242, %rd298;
	xor.b64  	%rd503, %rd249, %rd299;
	not.b64 	%rd300, %rd244;
	and.b64  	%rd301, %rd255, %rd300;
	xor.b64  	%rd522, %rd301, %rd253;
	not.b64 	%rd302, %rd255;
	and.b64  	%rd303, %rd248, %rd302;
	xor.b64  	%rd517, %rd303, %rd244;
	not.b64 	%rd304, %rd248;
	and.b64  	%rd305, %rd245, %rd304;
	xor.b64  	%rd512, %rd305, %rd255;
	not.b64 	%rd306, %rd245;
	and.b64  	%rd307, %rd253, %rd306;
	xor.b64  	%rd507, %rd307, %rd248;
	not.b64 	%rd308, %rd253;
	and.b64  	%rd309, %rd244, %rd308;
	xor.b64  	%rd502, %rd245, %rd309;
	ld.global.nc.u64 	%rd310, [%rd501];
	xor.b64  	%rd526, %rd261, %rd310;
	add.s64 	%rd501, %rd501, 8;
	add.s32 	%r552, %r552, 1;
	setp.ne.s32 	%p8, %r552, 24;
	@%p8 bra 	LBB0_4;

	add.u64 	%rd67, %SPL, 0;
	st.local.u64 	[%rd67], %rd526;
	st.local.u64 	[%rd67+8], %rd521;
	st.local.u64 	[%rd67+16], %rd516;
	st.local.u64 	[%rd67+24], %rd511;
	cvt.u16.u64 	%rs1, %rd526;
	and.b16  	%rs2, %rs1, 240;
	shr.u16 	%rs3, %rs2, 4;
	cvt.u32.u64 	%r150, %rd526;
	shr.u32 	%r151, %r150, 12;
	cvt.u32.u16 	%r152, %rs3;
	and.b32  	%r153, %r150, 15;
	prmt.b32 	%r154, %r153, %r152, 30212;
	shl.b32 	%r155, %r150, 4;
	and.b32  	%r156, %r155, 983040;
	or.b32  	%r157, %r154, %r156;
	shl.b32 	%r158, %r150, 16;
	and.b32  	%r159, %r158, 251658240;
	or.b32  	%r3, %r157, %r159;
	bfe.u32 	%r160, %r150, 20, 4;
	bfe.u32 	%r161, %r150, 16, 4;
	bfi.b32 	%r162, %r161, %r160, 8, 4;
	and.b32  	%r163, %r151, 983040;
	or.b32  	%r164, %r162, %r163;
	and.b32  	%r165, %r150, 251658240;
	or.b32  	%r4, %r164, %r165;
	shr.u64 	%rd313, %rd526, 32;
	cvt.u32.u64 	%r166, %rd313;
	shr.u64 	%rd314, %rd526, 36;
	cvt.u32.u64 	%r167, %rd314;
	and.b32  	%r168, %r167, 15;
	and.b32  	%r169, %r166, 15;
	shr.u64 	%rd315, %rd526, 40;
	cvt.u32.u64 	%r170, %rd315;
	shr.u64 	%rd316, %rd526, 44;
	cvt.u32.u64 	%r171, %rd316;
	bfi.b32 	%r172, %r169, %r168, 8, 4;
	shl.b32 	%r173, %r171, 16;
	and.b32  	%r174, %r173, 983040;
	or.b32  	%r175, %r172, %r174;
	shl.b32 	%r176, %r170, 24;
	and.b32  	%r177, %r176, 251658240;
	or.b32  	%r5, %r175, %r177;
	shr.u64 	%rd317, %rd526, 48;
	cvt.u32.u64 	%r178, %rd317;
	shr.u64 	%rd318, %rd526, 52;
	cvt.u32.u64 	%r179, %rd318;
	and.b32  	%r180, %r179, 15;
	and.b32  	%r181, %r178, 15;
	shr.u64 	%rd319, %rd526, 56;
	cvt.u32.u64 	%r182, %rd319;
	bfi.b32 	%r183, %r181, %r180, 8, 4;
	and.b32  	%r184, %r171, 983040;
	or.b32  	%r185, %r183, %r184;
	shl.b32 	%r186, %r182, 24;
	and.b32  	%r187, %r186, 251658240;
	or.b32  	%r6, %r185, %r187;
	cvt.u16.u64 	%rs4, %rd521;
	and.b16  	%rs5, %rs4, 240;
	shr.u16 	%rs6, %rs5, 4;
	cvt.u32.u64 	%r188, %rd521;
	shr.u32 	%r189, %r188, 12;
	cvt.u32.u16 	%r190, %rs6;
	and.b32  	%r191, %r188, 15;
	prmt.b32 	%r192, %r191, %r190, 30212;
	shl.b32 	%r193, %r188, 4;
	and.b32  	%r194, %r193, 983040;
	or.b32  	%r195, %r192, %r194;
	shl.b32 	%r196, %r188, 16;
	and.b32  	%r197, %r196, 251658240;
	or.b32  	%r7, %r195, %r197;
	bfe.u32 	%r198, %r188, 20, 4;
	bfe.u32 	%r199, %r188, 16, 4;
	bfi.b32 	%r200, %r199, %r198, 8, 4;
	and.b32  	%r201, %r189, 983040;
	or.b32  	%r202, %r200, %r201;
	and.b32  	%r203, %r188, 251658240;
	or.b32  	%r8, %r202, %r203;
	shr.u64 	%rd320, %rd521, 32;
	cvt.u32.u64 	%r204, %rd320;
	shr.u64 	%rd321, %rd521, 36;
	cvt.u32.u64 	%r205, %rd321;
	and.b32  	%r206, %r205, 15;
	and.b32  	%r207, %r204, 15;
	shr.u64 	%rd322, %rd521, 40;
	cvt.u32.u64 	%r208, %rd322;
	shr.u64 	%rd323, %rd521, 44;
	cvt.u32.u64 	%r209, %rd323;
	bfi.b32 	%r210, %r207, %r206, 8, 4;
	shl.b32 	%r211, %r209, 16;
	and.b32  	%r212, %r211, 983040;
	or.b32  	%r213, %r210, %r212;
	shl.b32 	%r214, %r208, 24;
	and.b32  	%r215, %r214, 251658240;
	or.b32  	%r9, %r213, %r215;
	shr.u64 	%rd324, %rd521, 48;
	cvt.u32.u64 	%r216, %rd324;
	shr.u64 	%rd325, %rd521, 52;
	cvt.u32.u64 	%r217, %rd325;
	and.b32  	%r218, %r217, 15;
	and.b32  	%r219, %r216, 15;
	shr.u64 	%rd326, %rd521, 56;
	cvt.u32.u64 	%r220, %rd326;
	bfi.b32 	%r221, %r219, %r218, 8, 4;
	and.b32  	%r222, %r209, 983040;
	or.b32  	%r223, %r221, %r222;
	shl.b32 	%r224, %r220, 24;
	and.b32  	%r225, %r224, 251658240;
	or.b32  	%r10, %r223, %r225;
	cvt.u16.u64 	%rs7, %rd516;
	and.b16  	%rs8, %rs7, 240;
	shr.u16 	%rs9, %rs8, 4;
	cvt.u32.u64 	%r226, %rd516;
	shr.u32 	%r227, %r226, 12;
	cvt.u32.u16 	%r228, %rs9;
	and.b32  	%r229, %r226, 15;
	prmt.b32 	%r230, %r229, %r228, 30212;
	shl.b32 	%r231, %r226, 4;
	and.b32  	%r232, %r231, 983040;
	or.b32  	%r233, %r230, %r232;
	shl.b32 	%r234, %r226, 16;
	and.b32  	%r235, %r234, 251658240;
	or.b32  	%r11, %r233, %r235;
	bfe.u32 	%r236, %r226, 20, 4;
	bfe.u32 	%r237, %r226, 16, 4;
	bfi.b32 	%r238, %r237, %r236, 8, 4;
	and.b32  	%r239, %r227, 983040;
	or.b32  	%r240, %r238, %r239;
	and.b32  	%r241, %r226, 251658240;
	or.b32  	%r12, %r240, %r241;
	shr.u64 	%rd327, %rd516, 32;
	cvt.u32.u64 	%r242, %rd327;
	shr.u64 	%rd328, %rd516, 36;
	cvt.u32.u64 	%r243, %rd328;
	and.b32  	%r244, %r243, 15;
	and.b32  	%r245, %r242, 15;
	shr.u64 	%rd329, %rd516, 40;
	cvt.u32.u64 	%r246, %rd329;
	shr.u64 	%rd330, %rd516, 44;
	cvt.u32.u64 	%r247, %rd330;
	bfi.b32 	%r248, %r245, %r244, 8, 4;
	shl.b32 	%r249, %r247, 16;
	and.b32  	%r250, %r249, 983040;
	or.b32  	%r251, %r248, %r250;
	shl.b32 	%r252, %r246, 24;
	and.b32  	%r253, %r252, 251658240;
	or.b32  	%r13, %r251, %r253;
	shr.u64 	%rd331, %rd516, 48;
	cvt.u32.u64 	%r254, %rd331;
	shr.u64 	%rd332, %rd516, 52;
	cvt.u32.u64 	%r255, %rd332;
	and.b32  	%r256, %r255, 15;
	and.b32  	%r257, %r254, 15;
	shr.u64 	%rd333, %rd516, 56;
	cvt.u32.u64 	%r258, %rd333;
	bfi.b32 	%r259, %r257, %r256, 8, 4;
	and.b32  	%r260, %r247, 983040;
	or.b32  	%r261, %r259, %r260;
	shl.b32 	%r262, %r258, 24;
	and.b32  	%r263, %r262, 251658240;
	or.b32  	%r14, %r261, %r263;
	cvt.u16.u64 	%rs10, %rd511;
	and.b16  	%rs11, %rs10, 240;
	shr.u16 	%rs12, %rs11, 4;
	cvt.u32.u64 	%r264, %rd511;
	shr.u32 	%r265, %r264, 12;
	cvt.u32.u16 	%r266, %rs12;
	and.b32  	%r267, %r264, 15;
	prmt.b32 	%r268, %r267, %r266, 30212;
	shl.b32 	%r269, %r264, 4;
	and.b32  	%r270, %r269, 983040;
	or.b32  	%r271, %r268, %r270;
	shl.b32 	%r272, %r264, 16;
	and.b32  	%r273, %r272, 251658240;
	or.b32  	%r15, %r271, %r273;
	bfe.u32 	%r274, %r264, 20, 4;
	bfe.u32 	%r275, %r264, 16, 4;
	bfi.b32 	%r276, %r275, %r274, 8, 4;
	and.b32  	%r277, %r265, 983040;
	or.b32  	%r278, %r276, %r277;
	and.b32  	%r279, %r264, 251658240;
	or.b32  	%r16, %r278, %r279;
	shr.u64 	%rd334, %rd511, 32;
	cvt.u32.u64 	%r280, %rd334;
	shr.u64 	%rd335, %rd511, 36;
	cvt.u32.u64 	%r281, %rd335;
	and.b32  	%r282, %r281, 15;
	and.b32  	%r283, %r280, 15;
	shr.u64 	%rd336, %rd511, 40;
	cvt.u32.u64 	%r284, %rd336;
	shr.u64 	%rd337, %rd511, 44;
	cvt.u32.u64 	%r285, %rd337;
	bfi.b32 	%r286, %r283, %r282, 8, 4;
	shl.b32 	%r287, %r285, 16;
	and.b32  	%r288, %r287, 983040;
	or.b32  	%r289, %r286, %r288;
	shl.b32 	%r290, %r284, 24;
	and.b32  	%r291, %r290, 251658240;
	or.b32  	%r17, %r289, %r291;
	shr.u64 	%rd338, %rd511, 48;
	cvt.u32.u64 	%r292, %rd338;
	shr.u64 	%rd339, %rd511, 52;
	cvt.u32.u64 	%r293, %rd339;
	and.b32  	%r294, %r293, 15;
	and.b32  	%r295, %r292, 15;
	shr.u64 	%rd340, %rd511, 56;
	cvt.u32.u64 	%r296, %rd340;
	bfi.b32 	%r297, %r295, %r294, 8, 4;
	and.b32  	%r298, %r285, 983040;
	or.b32  	%r299, %r297, %r298;
	shl.b32 	%r300, %r296, 24;
	and.b32  	%r301, %r300, 251658240;
	or.b32  	%r18, %r299, %r301;
	mov.u32 	%r149, 0;
	mov.u64 	%rd527, matrix;
	mov.u64 	%rd528, %rd67;
	mov.u32 	%r553, %r149;

LBB0_6:
	ld.const.u32 	%r303, [%rd527];
	// begin inline asm
	dp4a.u32.u32 %r302, %r303, %r3, %r149;
	// end inline asm
	ld.const.u32 	%r307, [%rd527+4];
	// begin inline asm
	dp4a.u32.u32 %r306, %r307, %r4, %r302;
	// end inline asm
	ld.const.u32 	%r311, [%rd527+8];
	// begin inline asm
	dp4a.u32.u32 %r310, %r311, %r5, %r306;
	// end inline asm
	ld.const.u32 	%r315, [%rd527+12];
	// begin inline asm
	dp4a.u32.u32 %r314, %r315, %r6, %r310;
	// end inline asm
	ld.const.u32 	%r319, [%rd527+16];
	// begin inline asm
	dp4a.u32.u32 %r318, %r319, %r7, %r314;
	// end inline asm
	ld.const.u32 	%r323, [%rd527+20];
	// begin inline asm
	dp4a.u32.u32 %r322, %r323, %r8, %r318;
	// end inline asm
	ld.const.u32 	%r327, [%rd527+24];
	// begin inline asm
	dp4a.u32.u32 %r326, %r327, %r9, %r322;
	// end inline asm
	ld.const.u32 	%r331, [%rd527+28];
	// begin inline asm
	dp4a.u32.u32 %r330, %r331, %r10, %r326;
	// end inline asm
	ld.const.u32 	%r335, [%rd527+32];
	// begin inline asm
	dp4a.u32.u32 %r334, %r335, %r11, %r330;
	// end inline asm
	ld.const.u32 	%r339, [%rd527+36];
	// begin inline asm
	dp4a.u32.u32 %r338, %r339, %r12, %r334;
	// end inline asm
	ld.const.u32 	%r343, [%rd527+40];
	// begin inline asm
	dp4a.u32.u32 %r342, %r343, %r13, %r338;
	// end inline asm
	ld.const.u32 	%r347, [%rd527+44];
	// begin inline asm
	dp4a.u32.u32 %r346, %r347, %r14, %r342;
	// end inline asm
	ld.const.u32 	%r351, [%rd527+48];
	// begin inline asm
	dp4a.u32.u32 %r350, %r351, %r15, %r346;
	// end inline asm
	ld.const.u32 	%r355, [%rd527+52];
	// begin inline asm
	dp4a.u32.u32 %r354, %r355, %r16, %r350;
	// end inline asm
	ld.const.u32 	%r359, [%rd527+56];
	// begin inline asm
	dp4a.u32.u32 %r358, %r359, %r17, %r354;
	// end inline asm
	ld.const.u32 	%r363, [%rd527+60];
	// begin inline asm
	dp4a.u32.u32 %r362, %r363, %r18, %r358;
	// end inline asm
	shr.u32 	%r434, %r362, 6;
	ld.const.u32 	%r367, [%rd527+64];
	// begin inline asm
	dp4a.u32.u32 %r366, %r367, %r3, %r149;
	// end inline asm
	ld.const.u32 	%r371, [%rd527+68];
	// begin inline asm
	dp4a.u32.u32 %r370, %r371, %r4, %r366;
	// end inline asm
	ld.const.u32 	%r375, [%rd527+72];
	// begin inline asm
	dp4a.u32.u32 %r374, %r375, %r5, %r370;
	// end inline asm
	ld.const.u32 	%r379, [%rd527+76];
	// begin inline asm
	dp4a.u32.u32 %r378, %r379, %r6, %r374;
	// end inline asm
	ld.const.u32 	%r383, [%rd527+80];
	// begin inline asm
	dp4a.u32.u32 %r382, %r383, %r7, %r378;
	// end inline asm
	ld.const.u32 	%r387, [%rd527+84];
	// begin inline asm
	dp4a.u32.u32 %r386, %r387, %r8, %r382;
	// end inline asm
	ld.const.u32 	%r391, [%rd527+88];
	// begin inline asm
	dp4a.u32.u32 %r390, %r391, %r9, %r386;
	// end inline asm
	ld.const.u32 	%r395, [%rd527+92];
	// begin inline asm
	dp4a.u32.u32 %r394, %r395, %r10, %r390;
	// end inline asm
	ld.const.u32 	%r399, [%rd527+96];
	// begin inline asm
	dp4a.u32.u32 %r398, %r399, %r11, %r394;
	// end inline asm
	ld.const.u32 	%r403, [%rd527+100];
	// begin inline asm
	dp4a.u32.u32 %r402, %r403, %r12, %r398;
	// end inline asm
	ld.const.u32 	%r407, [%rd527+104];
	// begin inline asm
	dp4a.u32.u32 %r406, %r407, %r13, %r402;
	// end inline asm
	ld.const.u32 	%r411, [%rd527+108];
	// begin inline asm
	dp4a.u32.u32 %r410, %r411, %r14, %r406;
	// end inline asm
	ld.const.u32 	%r415, [%rd527+112];
	// begin inline asm
	dp4a.u32.u32 %r414, %r415, %r15, %r410;
	// end inline asm
	ld.const.u32 	%r419, [%rd527+116];
	// begin inline asm
	dp4a.u32.u32 %r418, %r419, %r16, %r414;
	// end inline asm
	ld.const.u32 	%r423, [%rd527+120];
	// begin inline asm
	dp4a.u32.u32 %r422, %r423, %r17, %r418;
	// end inline asm
	ld.const.u32 	%r427, [%rd527+124];
	// begin inline asm
	dp4a.u32.u32 %r426, %r427, %r18, %r422;
	// end inline asm
	and.b32  	%r431, %r434, 240;
	shr.u32 	%r432, %r426, 10;
	ld.local.u8 	%r433, [%rd528];
	// begin inline asm
	lop3.b32 %r430, %r431, %r432, %r433, 0x56;
	// end inline asm
	st.local.u8 	[%rd528], %r430;
	add.s64 	%rd528, %rd528, 1;
	add.s64 	%rd527, %rd527, 128;
	add.s32 	%r553, %r553, 1;
	setp.ne.s32 	%p9, %r553, 32;
	@%p9 bra 	LBB0_6;

	ld.local.u64 	%rd363, [%rd67];
	xor.b64  	%rd126, %rd363, 4239941492252378377;
	ld.local.u64 	%rd364, [%rd67+8];
	xor.b64  	%rd549, %rd364, 8746723911537738262;
	ld.local.u64 	%rd365, [%rd67+16];
	xor.b64  	%rd544, %rd365, 8796936657246353646;
	ld.local.u64 	%rd366, [%rd67+24];
	xor.b64  	%rd539, %rd366, 1272090201925444760;
	mov.u32 	%r554, 0;
	mov.u64 	%rd553, 8270816933120786537;
	mov.u64 	%rd552, -850687345431043546;
	mov.u64 	%rd551, 8596393687355028144;
	mov.u64 	%rd550, -4073852189716399785;
	mov.u64 	%rd548, -4539347866060507718;
	mov.u64 	%rd547, -3233781605604422593;
	mov.u64 	%rd546, 570094237299545110;
	mov.u64 	%rd545, 5171152063242093102;
	mov.u64 	%rd543, 6782861118970774626;
	mov.u64 	%rd542, 7812475424661425213;
	mov.u64 	%rd541, 9119540418498120711;
	mov.u64 	%rd540, -7873636174015165430;
	mov.u64 	%rd538, -9207053471590684088;
	mov.u64 	%rd537, 3370482334374859748;
	mov.u64 	%rd536, -1544774801229058759;
	mov.u64 	%rd535, 6096431547456407061;
	mov.u64 	%rd534, -1792185402154627366;
	mov.u64 	%rd533, -6864424130110145268;
	mov.u64 	%rd532, 5690099369266491460;
	mov.u64 	%rd531, -5074726839974049192;
	mov.u64 	%rd530, 1592359455985097269;
	mov.u64 	%rd529, RC;

LBB0_8:
	xor.b64  	%rd367, %rd553, %rd126;
	xor.b64  	%rd368, %rd367, %rd552;
	xor.b64  	%rd369, %rd368, %rd551;
	xor.b64  	%rd370, %rd369, %rd550;
	xor.b64  	%rd371, %rd548, %rd549;
	xor.b64  	%rd372, %rd371, %rd547;
	xor.b64  	%rd373, %rd372, %rd546;
	xor.b64  	%rd374, %rd373, %rd545;
	xor.b64  	%rd375, %rd543, %rd544;
	xor.b64  	%rd376, %rd375, %rd542;
	xor.b64  	%rd377, %rd376, %rd541;
	xor.b64  	%rd378, %rd377, %rd540;
	xor.b64  	%rd379, %rd538, %rd539;
	xor.b64  	%rd380, %rd379, %rd537;
	xor.b64  	%rd381, %rd380, %rd536;
	xor.b64  	%rd382, %rd381, %rd535;
	xor.b64  	%rd383, %rd533, %rd534;
	xor.b64  	%rd384, %rd383, %rd532;
	xor.b64  	%rd385, %rd384, %rd531;
	xor.b64  	%rd386, %rd385, %rd530;
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r436}, %rd374;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%r437,%dummy}, %rd374;
	}
	shf.l.wrap.b32 	%r438, %r437, %r436, 1;
	shf.l.wrap.b32 	%r439, %r436, %r437, 1;
	mov.b64 	%rd387, {%r439, %r438};
	xor.b64  	%rd388, %rd386, %rd387;
	xor.b64  	%rd389, %rd388, %rd126;
	xor.b64  	%rd390, %rd553, %rd388;
	xor.b64  	%rd391, %rd552, %rd388;
	xor.b64  	%rd392, %rd551, %rd388;
	xor.b64  	%rd393, %rd550, %rd388;
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r440}, %rd378;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%r441,%dummy}, %rd378;
	}
	shf.l.wrap.b32 	%r442, %r441, %r440, 1;
	shf.l.wrap.b32 	%r443, %r440, %r441, 1;
	mov.b64 	%rd394, {%r443, %r442};
	xor.b64  	%rd395, %rd394, %rd370;
	xor.b64  	%rd396, %rd549, %rd395;
	xor.b64  	%rd397, %rd548, %rd395;
	xor.b64  	%rd398, %rd547, %rd395;
	xor.b64  	%rd399, %rd546, %rd395;
	xor.b64  	%rd400, %rd545, %rd395;
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r444}, %rd382;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%r445,%dummy}, %rd382;
	}
	shf.l.wrap.b32 	%r446, %r445, %r444, 1;
	shf.l.wrap.b32 	%r447, %r444, %r445, 1;
	mov.b64 	%rd401, {%r447, %r446};
	xor.b64  	%rd402, %rd401, %rd374;
	xor.b64  	%rd403, %rd544, %rd402;
	xor.b64  	%rd404, %rd543, %rd402;
	xor.b64  	%rd405, %rd542, %rd402;
	xor.b64  	%rd406, %rd541, %rd402;
	xor.b64  	%rd407, %rd540, %rd402;
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r448}, %rd386;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%r449,%dummy}, %rd386;
	}
	shf.l.wrap.b32 	%r450, %r449, %r448, 1;
	shf.l.wrap.b32 	%r451, %r448, %r449, 1;
	mov.b64 	%rd408, {%r451, %r450};
	xor.b64  	%rd409, %rd408, %rd378;
	xor.b64  	%rd410, %rd539, %rd409;
	xor.b64  	%rd411, %rd538, %rd409;
	xor.b64  	%rd412, %rd537, %rd409;
	xor.b64  	%rd413, %rd536, %rd409;
	xor.b64  	%rd414, %rd535, %rd409;
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r452}, %rd370;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%r453,%dummy}, %rd370;
	}
	shf.l.wrap.b32 	%r454, %r453, %r452, 1;
	shf.l.wrap.b32 	%r455, %r452, %r453, 1;
	mov.b64 	%rd415, {%r455, %r454};
	xor.b64  	%rd416, %rd382, %rd415;
	xor.b64  	%rd417, %rd534, %rd416;
	xor.b64  	%rd418, %rd533, %rd416;
	xor.b64  	%rd419, %rd532, %rd416;
	xor.b64  	%rd420, %rd531, %rd416;
	xor.b64  	%rd421, %rd530, %rd416;
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r456}, %rd396;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%r457,%dummy}, %rd396;
	}
	shf.l.wrap.b32 	%r458, %r457, %r456, 1;
	shf.l.wrap.b32 	%r459, %r456, %r457, 1;
	mov.b64 	%rd422, {%r459, %r458};
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r460}, %rd391;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%r461,%dummy}, %rd391;
	}
	shf.l.wrap.b32 	%r462, %r461, %r460, 3;
	shf.l.wrap.b32 	%r463, %r460, %r461, 3;
	mov.b64 	%rd423, {%r463, %r462};
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r464}, %rd404;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%r465,%dummy}, %rd404;
	}
	shf.l.wrap.b32 	%r466, %r465, %r464, 6;
	shf.l.wrap.b32 	%r467, %r464, %r465, 6;
	mov.b64 	%rd424, {%r467, %r466};
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r468}, %rd398;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%r469,%dummy}, %rd398;
	}
	shf.l.wrap.b32 	%r470, %r469, %r468, 10;
	shf.l.wrap.b32 	%r471, %r468, %r469, 10;
	mov.b64 	%rd425, {%r471, %r470};
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r472}, %rd406;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%r473,%dummy}, %rd406;
	}
	shf.l.wrap.b32 	%r474, %r473, %r472, 15;
	shf.l.wrap.b32 	%r475, %r472, %r473, 15;
	mov.b64 	%rd426, {%r475, %r474};
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r476}, %rd413;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%r477,%dummy}, %rd413;
	}
	shf.l.wrap.b32 	%r478, %r477, %r476, 21;
	shf.l.wrap.b32 	%r479, %r476, %r477, 21;
	mov.b64 	%rd427, {%r479, %r478};
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r480}, %rd410;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%r481,%dummy}, %rd410;
	}
	shf.l.wrap.b32 	%r482, %r481, %r480, 28;
	shf.l.wrap.b32 	%r483, %r480, %r481, 28;
	mov.b64 	%rd428, {%r483, %r482};
	{
	.reg .b32 %dummy;
	mov.b64 	{%r484,%dummy}, %rd390;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r485}, %rd390;
	}
	shf.r.wrap.b32 	%r486, %r485, %r484, 28;
	shf.r.wrap.b32 	%r487, %r484, %r485, 28;
	mov.b64 	%rd429, {%r487, %r486};
	{
	.reg .b32 %dummy;
	mov.b64 	{%r488,%dummy}, %rd399;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r489}, %rd399;
	}
	shf.r.wrap.b32 	%r490, %r489, %r488, 19;
	shf.r.wrap.b32 	%r491, %r488, %r489, 19;
	mov.b64 	%rd430, {%r491, %r490};
	{
	.reg .b32 %dummy;
	mov.b64 	{%r492,%dummy}, %rd411;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r493}, %rd411;
	}
	shf.r.wrap.b32 	%r494, %r493, %r492, 9;
	shf.r.wrap.b32 	%r495, %r492, %r493, 9;
	mov.b64 	%rd431, {%r495, %r494};
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r496}, %rd400;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%r497,%dummy}, %rd400;
	}
	shf.l.wrap.b32 	%r498, %r497, %r496, 2;
	shf.l.wrap.b32 	%r499, %r496, %r497, 2;
	mov.b64 	%rd432, {%r499, %r498};
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r500}, %rd421;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%r501,%dummy}, %rd421;
	}
	shf.l.wrap.b32 	%r502, %r501, %r500, 14;
	shf.l.wrap.b32 	%r503, %r500, %r501, 14;
	mov.b64 	%rd433, {%r503, %r502};
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r504}, %rd417;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%r505,%dummy}, %rd417;
	}
	shf.l.wrap.b32 	%r506, %r505, %r504, 27;
	shf.l.wrap.b32 	%r507, %r504, %r505, 27;
	mov.b64 	%rd434, {%r507, %r506};
	{
	.reg .b32 %dummy;
	mov.b64 	{%r508,%dummy}, %rd392;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r509}, %rd392;
	}
	shf.r.wrap.b32 	%r510, %r509, %r508, 23;
	shf.r.wrap.b32 	%r511, %r508, %r509, 23;
	mov.b64 	%rd435, {%r511, %r510};
	{
	.reg .b32 %dummy;
	mov.b64 	{%r512,%dummy}, %rd414;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r513}, %rd414;
	}
	shf.r.wrap.b32 	%r514, %r513, %r512, 8;
	shf.r.wrap.b32 	%r515, %r512, %r513, 8;
	mov.b64 	%rd436, {%r515, %r514};
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r516}, %rd420;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%r517,%dummy}, %rd420;
	}
	shf.l.wrap.b32 	%r518, %r517, %r516, 8;
	shf.l.wrap.b32 	%r519, %r516, %r517, 8;
	mov.b64 	%rd437, {%r519, %r518};
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r520}, %rd412;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%r521,%dummy}, %rd412;
	}
	shf.l.wrap.b32 	%r522, %r521, %r520, 25;
	shf.l.wrap.b32 	%r523, %r520, %r521, 25;
	mov.b64 	%rd438, {%r523, %r522};
	{
	.reg .b32 %dummy;
	mov.b64 	{%r524,%dummy}, %rd405;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r525}, %rd405;
	}
	shf.r.wrap.b32 	%r526, %r525, %r524, 21;
	shf.r.wrap.b32 	%r527, %r524, %r525, 21;
	mov.b64 	%rd439, {%r527, %r526};
	{
	.reg .b32 %dummy;
	mov.b64 	{%r528,%dummy}, %rd403;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r529}, %rd403;
	}
	shf.r.wrap.b32 	%r530, %r529, %r528, 2;
	shf.r.wrap.b32 	%r531, %r528, %r529, 2;
	mov.b64 	%rd440, {%r531, %r530};
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r532}, %rd393;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%r533,%dummy}, %rd393;
	}
	shf.l.wrap.b32 	%r534, %r533, %r532, 18;
	shf.l.wrap.b32 	%r535, %r532, %r533, 18;
	mov.b64 	%rd441, {%r535, %r534};
	{
	.reg .b32 %dummy;
	mov.b64 	{%r536,%dummy}, %rd419;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r537}, %rd419;
	}
	shf.r.wrap.b32 	%r538, %r537, %r536, 25;
	shf.r.wrap.b32 	%r539, %r536, %r537, 25;
	mov.b64 	%rd442, {%r539, %r538};
	{
	.reg .b32 %dummy;
	mov.b64 	{%r540,%dummy}, %rd407;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r541}, %rd407;
	}
	shf.r.wrap.b32 	%r542, %r541, %r540, 3;
	shf.r.wrap.b32 	%r543, %r540, %r541, 3;
	mov.b64 	%rd443, {%r543, %r542};
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r544}, %rd418;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%r545,%dummy}, %rd418;
	}
	shf.l.wrap.b32 	%r546, %r545, %r544, 20;
	shf.l.wrap.b32 	%r547, %r544, %r545, 20;
	mov.b64 	%rd444, {%r547, %r546};
	{
	.reg .b32 %dummy;
	mov.b64 	{%r548,%dummy}, %rd397;
	}
	{
	.reg .b32 %dummy;
	mov.b64 	{%dummy,%r549}, %rd397;
	}
	shf.r.wrap.b32 	%r550, %r549, %r548, 20;
	shf.r.wrap.b32 	%r551, %r548, %r549, 20;
	mov.b64 	%rd445, {%r551, %r550};
	not.b64 	%rd446, %rd445;
	and.b64  	%rd447, %rd439, %rd446;
	xor.b64  	%rd448, %rd447, %rd389;
	not.b64 	%rd449, %rd439;
	and.b64  	%rd450, %rd427, %rd449;
	xor.b64  	%rd549, %rd450, %rd445;
	not.b64 	%rd451, %rd427;
	and.b64  	%rd452, %rd433, %rd451;
	xor.b64  	%rd544, %rd452, %rd439;
	not.b64 	%rd453, %rd433;
	and.b64  	%rd454, %rd389, %rd453;
	xor.b64  	%rd539, %rd454, %rd427;
	not.b64 	%rd455, %rd389;
	and.b64  	%rd456, %rd445, %rd455;
	xor.b64  	%rd534, %rd433, %rd456;
	not.b64 	%rd457, %rd444;
	and.b64  	%rd458, %rd423, %rd457;
	xor.b64  	%rd553, %rd458, %rd428;
	not.b64 	%rd459, %rd423;
	and.b64  	%rd460, %rd430, %rd459;
	xor.b64  	%rd548, %rd460, %rd444;
	not.b64 	%rd461, %rd430;
	and.b64  	%rd462, %rd443, %rd461;
	xor.b64  	%rd543, %rd462, %rd423;
	not.b64 	%rd463, %rd443;
	and.b64  	%rd464, %rd428, %rd463;
	xor.b64  	%rd538, %rd464, %rd430;
	not.b64 	%rd465, %rd428;
	and.b64  	%rd466, %rd444, %rd465;
	xor.b64  	%rd533, %rd443, %rd466;
	not.b64 	%rd467, %rd424;
	and.b64  	%rd468, %rd438, %rd467;
	xor.b64  	%rd552, %rd468, %rd422;
	not.b64 	%rd469, %rd438;
	and.b64  	%rd470, %rd437, %rd469;
	xor.b64  	%rd547, %rd470, %rd424;
	not.b64 	%rd471, %rd437;
	and.b64  	%rd472, %rd441, %rd471;
	xor.b64  	%rd542, %rd472, %rd438;
	not.b64 	%rd473, %rd441;
	and.b64  	%rd474, %rd422, %rd473;
	xor.b64  	%rd537, %rd474, %rd437;
	not.b64 	%rd475, %rd422;
	and.b64  	%rd476, %rd424, %rd475;
	xor.b64  	%rd532, %rd441, %rd476;
	not.b64 	%rd477, %rd429;
	and.b64  	%rd478, %rd425, %rd477;
	xor.b64  	%rd551, %rd478, %rd434;
	not.b64 	%rd479, %rd425;
	and.b64  	%rd480, %rd426, %rd479;
	xor.b64  	%rd546, %rd480, %rd429;
	not.b64 	%rd481, %rd426;
	and.b64  	%rd482, %rd436, %rd481;
	xor.b64  	%rd541, %rd482, %rd425;
	not.b64 	%rd483, %rd436;
	and.b64  	%rd484, %rd434, %rd483;
	xor.b64  	%rd536, %rd484, %rd426;
	not.b64 	%rd485, %rd434;
	and.b64  	%rd486, %rd429, %rd485;
	xor.b64  	%rd531, %rd436, %rd486;
	not.b64 	%rd487, %rd431;
	and.b64  	%rd488, %rd442, %rd487;
	xor.b64  	%rd550, %rd488, %rd440;
	not.b64 	%rd489, %rd442;
	and.b64  	%rd490, %rd435, %rd489;
	xor.b64  	%rd545, %rd490, %rd431;
	not.b64 	%rd491, %rd435;
	and.b64  	%rd492, %rd432, %rd491;
	xor.b64  	%rd540, %rd492, %rd442;
	not.b64 	%rd493, %rd432;
	and.b64  	%rd494, %rd440, %rd493;
	xor.b64  	%rd535, %rd494, %rd435;
	not.b64 	%rd495, %rd440;
	and.b64  	%rd496, %rd431, %rd495;
	xor.b64  	%rd530, %rd432, %rd496;
	ld.global.nc.u64 	%rd497, [%rd529];
	xor.b64  	%rd126, %rd448, %rd497;
	add.s64 	%rd529, %rd529, 8;
	add.s32 	%r554, %r554, 1;
	setp.ne.s32 	%p10, %r554, 24;
	@%p10 bra 	LBB0_8;

	st.local.u64 	[%rd67], %rd126;
	st.local.u64 	[%rd67+8], %rd549;
	st.local.u64 	[%rd67+16], %rd544;
	st.local.u64 	[%rd67+24], %rd539;
	ld.const.u64 	%rd128, [target+24];
	setp.eq.s64 	%p11, %rd539, %rd128;
	@%p11 bra 	LBB0_11;
	bra.uni 	LBB0_10;

LBB0_11:
	ld.const.u64 	%rd129, [target+16];
	setp.eq.s64 	%p12, %rd544, %rd129;
	@%p12 bra 	LBB0_13;
	bra.uni 	LBB0_12;

LBB0_13:
	ld.const.u64 	%rd130, [target+8];
	setp.eq.s64 	%p13, %rd549, %rd130;
	@%p13 bra 	LBB0_15;
	bra.uni 	LBB0_14;

LBB0_15:
	ld.const.u64 	%rd498, [target];
	setp.lt.u64 	%p15, %rd126, %rd498;
	bra.uni 	LBB0_16;

LBB0_10:
	setp.lt.u64 	%p15, %rd539, %rd128;
	bra.uni 	LBB0_16;

LBB0_12:
	setp.lt.u64 	%p15, %rd544, %rd129;
	bra.uni 	LBB0_16;

LBB0_14:
	setp.lt.u64 	%p15, %rd549, %rd130;

LBB0_16:
	not.pred 	%p14, %p15;
	@%p14 bra 	LBB0_18;

	mov.u64 	%rd499, 0;
	atom.global.cas.b64 	%rd500, [%rd1], %rd499, %rd3;

LBB0_18:
	ret;

}

